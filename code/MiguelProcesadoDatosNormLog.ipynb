{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1af132f-3f24-42bd-944b-9b3459470437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparaci√≥n de los dos conjuntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b5ccba-d041-49ff-8c08-56e8b577bd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceso completado.\n",
      "Pares encontrados y copiados: 16114\n",
      "Archivos DUD sin par ACS: 5886\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Rutas base\n",
    "base_dud = \"../data/raw/dud\"\n",
    "base_acs = \"../data/raw/acs/\"\n",
    "output_dud = \"../data/process/dud/\"\n",
    "output_acs = \"../data/process/acs/\"\n",
    "\n",
    "# Asegurar que las carpetas de salida existan\n",
    "os.makedirs(output_dud, exist_ok=True)\n",
    "os.makedirs(output_acs, exist_ok=True)\n",
    "\n",
    "# Subcarpetas que se deben explorar\n",
    "subdirs = [\"n1\", \"n2\"]\n",
    "\n",
    "paired_count = 0\n",
    "missing_count = 0\n",
    "\n",
    "for subdir in subdirs:\n",
    "    dud_dir = os.path.join(base_dud, subdir)\n",
    "    acs_dir = os.path.join(base_acs, subdir)\n",
    "    \n",
    "    if not os.path.isdir(dud_dir) or not os.path.isdir(acs_dir):\n",
    "        print(f\"Advertencia: faltan carpetas en {subdir}\")\n",
    "        continue\n",
    "    \n",
    "    dud_files = [f for f in os.listdir(dud_dir) if f.endswith(\".fits\")]\n",
    "    \n",
    "    for dud_file in dud_files:\n",
    "        # Construir el nombre correspondiente en ACS\n",
    "        base_name = dud_file.replace(\".fits\", \"\")\n",
    "        acs_file = base_name + \"_ACS.fits\"\n",
    "\n",
    "        dud_path = os.path.join(dud_dir, dud_file)\n",
    "        acs_path = os.path.join(acs_dir, acs_file)\n",
    "\n",
    "        if os.path.exists(acs_path):\n",
    "            # Copiar ambos archivos a su nueva ubicaci√≥n\n",
    "            shutil.copy2(dud_path, os.path.join(output_dud, dud_file))\n",
    "            shutil.copy2(acs_path, os.path.join(output_acs, acs_file))\n",
    "            paired_count += 1\n",
    "        else:\n",
    "            missing_count += 1\n",
    "\n",
    "print(f\"\\nProceso completado.\")\n",
    "print(f\"Pares encontrados y copiados: {paired_count}\")\n",
    "print(f\"Archivos DUD sin par ACS: {missing_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb55a2-4a70-411e-ae94-ab7a79e7c7b5",
   "metadata": {},
   "source": [
    "# Procesado y normalizacion de los datos para su uso por un modelo\n",
    "No se ha hecho, pero estaria bien:  (Si se ha hecho una normalizaci√≥n global)\n",
    "\n",
    "üìâ 3. Normalizaci√≥n estad√≠stica por imagen  \n",
    "\n",
    "Justificaci√≥n: La normalizaci√≥n min-max global es buena para mantener una escala uniforme, pero si hay alta variabilidad entre im√°genes, podr√≠as experimentar con:  \n",
    "- Normalizaci√≥n por imagen individual usando media y desviaci√≥n t√≠pica (z-score).  \n",
    "- O bien escalar entre 0 y 1 por imagen, especialmente √∫til si el rango din√°mico var√≠a mucho entre ejemplos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98a9fc6-8417-4612-b8e8-427ea1f8b220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando logar√≠tmicamente y guardando como .npy en carpetas separadas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñä                                                                            | 162/16114 [00:00<01:34, 168.23it/s]C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_21144\\328024152.py:19: RuntimeWarning: invalid value encountered in log1p\n",
      "  log_transformed_data = np.log1p(image_data_float)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [01:40<00:00, 159.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total de pares procesados y guardados: 16114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directorios de entrada\n",
    "dud_dir = \"../data/process/dud/\"\n",
    "acs_dir = \"../data/process/acs/\"\n",
    "\n",
    "# Directorios de salida\n",
    "output_base = \"../data/log/normalized/\"\n",
    "dud_output_dir = os.path.join(output_base, \"dud\")\n",
    "acs_output_dir = os.path.join(output_base, \"acs\")\n",
    "os.makedirs(dud_output_dir, exist_ok=True)\n",
    "os.makedirs(acs_output_dir, exist_ok=True)\n",
    "\n",
    "def normalize_log_image(image_data):\n",
    "    image_data_float = image_data.astype(np.float32)\n",
    "    log_transformed_data = np.log1p(image_data_float)\n",
    "\n",
    "    min_val = np.min(log_transformed_data)\n",
    "    max_val = np.max(log_transformed_data)\n",
    "\n",
    "    if max_val == min_val:\n",
    "        return np.zeros_like(log_transformed_data)\n",
    "    \n",
    "    normalized_data = (log_transformed_data - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return normalized_data\n",
    "\n",
    "dud_files = [f for f in os.listdir(dud_dir) if f.endswith(\".fits\")]\n",
    "paired_count = 0\n",
    "\n",
    "print(\"Normalizando logar√≠tmicamente y guardando como .npy en carpetas separadas...\")\n",
    "for dud_file in tqdm(dud_files):\n",
    "    base_name = dud_file.replace(\".fits\", \"\")\n",
    "    acs_file = base_name + \"_ACS.fits\"\n",
    "\n",
    "    dud_path = os.path.join(dud_dir, dud_file)\n",
    "    acs_path = os.path.join(acs_dir, acs_file)\n",
    "\n",
    "    if not os.path.exists(acs_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with fits.open(dud_path, verify='silentfix') as dud_hdul:\n",
    "            dud_data = next(hdu.data for hdu in dud_hdul if hdu.data is not None)\n",
    "            dud_data = normalize_log_image(dud_data)\n",
    "\n",
    "        with fits.open(acs_path, verify='silentfix') as acs_hdul:\n",
    "            acs_data = next(hdu.data for hdu in acs_hdul if hdu.data is not None)\n",
    "            acs_data = normalize_log_image(acs_data)\n",
    "\n",
    "        np.save(os.path.join(dud_output_dir, base_name + \".npy\"), dud_data)\n",
    "        np.save(os.path.join(acs_output_dir, base_name + \".npy\"), acs_data)\n",
    "\n",
    "        paired_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {dud_file}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total de pares procesados y guardados: {paired_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2deee0-d989-4117-996b-aa0f56f619d3",
   "metadata": {},
   "source": [
    "S√≠, otra opci√≥n que a menudo se considera superior para el downsampling (reducci√≥n de tama√±o) en t√©rminos de calidad de imagen es la interpolaci√≥n Lanczos.\n",
    "\n",
    "Lanczos tiende a producir resultados m√°s n√≠tidos y con menos desenfoque que la bic√∫bica, lo que puede ser beneficioso para preservar los detalles finos en las galaxias. Sin embargo, a veces puede introducir un efecto de \"ringing\" (artefactos de anillo) alrededor de bordes muy contrastados, aunque esto rara vez es un problema mayor para la mayor√≠a de las im√°genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e07ebd-a557-4bf0-a444-d189d73d1a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PASO 2: Aplicando Padding/Reescalado Lanczos a datos .npy normalizados ---\n",
      "Directorio de salida '../data/log/normalized/dud_processed_lanczos' creado.\n",
      "\n",
      "Procesando im√°genes .npy en '../data/log/normalized/dud' a 96x96 (padding/reescalado Lanczos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [00:17<00:00, 937.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de salida '../data/log/normalized/acs_processed_lanczos' creado.\n",
      "\n",
      "Procesando im√°genes .npy en '../data/log/normalized/acs' a 384x384 (padding/reescalado Lanczos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [00:27<00:00, 590.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ ¬°Procesamiento completo: normalizaci√≥n y reescalado Lanczos aplicados!\n",
      "Im√°genes DUD procesadas guardadas en: ../data/log/normalized/dud_processed_lanczos\n",
      "Im√°genes ACS procesadas guardadas en: ../data/log/normalized/acs_processed_lanczos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pading y resice de las grandes\n",
    "import os\n",
    "import numpy as np\n",
    "from astropy.io import fits # Todav√≠a se usa si quieres el header, pero no para leer la imagen\n",
    "from scipy.ndimage import zoom\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_normalized_npy(input_npy_dir, output_npy_dir, target_resolution):\n",
    "    \"\"\"\n",
    "    Aplica padding a im√°genes .npy peque√±as y reescalado Lanczos a im√°genes .npy grandes\n",
    "    para alcanzar una resoluci√≥n cuadrada objetivo. Guarda las im√°genes resultantes\n",
    "    como archivos .npy.\n",
    "\n",
    "    Args:\n",
    "        input_npy_dir (str): Directorio de entrada con las im√°genes .npy normalizadas.\n",
    "        output_npy_dir (str): Directorio donde se guardar√°n las im√°genes .npy procesadas.\n",
    "        target_resolution (int): La resoluci√≥n cuadrada deseada (ej. 21 para 21x21).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_npy_dir):\n",
    "        print(f\"Error: El directorio de entrada '{input_npy_dir}' no existe.\")\n",
    "        return\n",
    "    if not os.path.exists(output_npy_dir):\n",
    "        os.makedirs(output_npy_dir)\n",
    "        print(f\"Directorio de salida '{output_npy_dir}' creado.\")\n",
    "\n",
    "    npy_files = [f for f in os.listdir(input_npy_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "    if not npy_files:\n",
    "        print(f\"No se encontraron archivos .npy en '{input_npy_dir}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcesando im√°genes .npy en '{input_npy_dir}' a {target_resolution}x{target_resolution} (padding/reescalado Lanczos)...\")\n",
    "\n",
    "    for filename in tqdm(npy_files):\n",
    "        input_path = os.path.join(input_npy_dir, filename)\n",
    "        output_path = os.path.join(output_npy_dir, filename) # Se mantiene el mismo nombre\n",
    "\n",
    "        try:\n",
    "            original_data = np.load(input_path)\n",
    "            original_height, original_width = original_data.shape\n",
    "            original_dtype = original_data.dtype # Usamos el dtype original, que ser√° float32\n",
    "\n",
    "            processed_data = np.zeros((target_resolution, target_resolution), dtype=original_dtype)\n",
    "\n",
    "            # --- L√≥gica de procesamiento ---\n",
    "            if original_width < target_resolution or original_height < target_resolution:\n",
    "                # Aplicar padding para im√°genes peque√±as\n",
    "                start_y = (target_resolution - original_height) // 2\n",
    "                start_x = (target_resolution - original_width) // 2\n",
    "                \n",
    "                processed_data[start_y : start_y + original_height,\n",
    "                               start_x : start_x + original_width] = original_data\n",
    "            else:\n",
    "                # Reescalar Lanczos para im√°genes grandes o de tama√±o similar\n",
    "                zoom_factor_h = target_resolution / original_height\n",
    "                zoom_factor_w = target_resolution / original_width\n",
    "\n",
    "                # No necesitamos convertir a float32 aqu√≠ porque ya est√°n normalizadas (float32)\n",
    "                zoomed_data = zoom(original_data, (zoom_factor_h, zoom_factor_w), order=5) \n",
    "                \n",
    "                # Aseguramos que la imagen reescalada tenga exactamente la resoluci√≥n objetivo\n",
    "                current_h, current_w = zoomed_data.shape\n",
    "                \n",
    "                if current_h != target_resolution or current_w != target_resolution:\n",
    "                    final_zoomed_data = np.zeros((target_resolution, target_resolution), dtype=original_dtype)\n",
    "                    \n",
    "                    insert_h = min(target_resolution, current_h)\n",
    "                    insert_w = min(target_resolution, current_w)\n",
    "\n",
    "                    start_y_zoom = (target_resolution - insert_h) // 2\n",
    "                    start_x_zoom = (target_resolution - insert_w) // 2\n",
    "\n",
    "                    src_start_y = (current_h - insert_h) // 2\n",
    "                    src_start_x = (current_w - insert_w) // 2\n",
    "\n",
    "                    final_zoomed_data[start_y_zoom : start_y_zoom + insert_h,\n",
    "                                      start_x_zoom : start_x_zoom + insert_w] = \\\n",
    "                                      zoomed_data[src_start_y : src_start_y + insert_h,\n",
    "                                                  src_start_x : src_start_x + insert_w]\n",
    "                    processed_data = final_zoomed_data.astype(original_dtype)\n",
    "                else:\n",
    "                    processed_data = zoomed_data.astype(original_dtype)\n",
    "            \n",
    "            # Guardar el archivo .npy procesado\n",
    "            np.save(output_path, processed_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {filename} (reescalado Lanczos): {e}\")\n",
    "\n",
    "# Directorios de salida para los datos normalizados (.npy)\n",
    "normalized_base_dir = \"../data/log/normalized/\"\n",
    "dud_normalized_dir = os.path.join(normalized_base_dir, \"dud\")\n",
    "acs_normalized_dir = os.path.join(normalized_base_dir, \"acs\")\n",
    "\n",
    "# Define los directorios de salida para las im√°genes .npy procesadas\n",
    "# (despu√©s de normalizaci√≥n y reescalado/padding)\n",
    "processed_dud_npy_dir = os.path.join(normalized_base_dir, \"dud_processed_lanczos\")\n",
    "processed_acs_npy_dir = os.path.join(normalized_base_dir, \"acs_processed_lanczos\")\n",
    "\n",
    "# Definir resoluciones objetivo (las mismas que antes)\n",
    "target_dud_resolution = 96\n",
    "target_acs_resolution = 384\n",
    "\n",
    "print(\"\\n--- PASO 2: Aplicando Padding/Reescalado Lanczos a datos .npy normalizados ---\")\n",
    "\n",
    "# Procesar las im√°genes DUD normalizadas\n",
    "process_normalized_npy(dud_normalized_dir, processed_dud_npy_dir, target_dud_resolution)\n",
    "\n",
    "# Procesar las im√°genes ACS normalizadas\n",
    "process_normalized_npy(acs_normalized_dir, processed_acs_npy_dir, target_acs_resolution)\n",
    "\n",
    "print(\"\\nüéâ ¬°Procesamiento completo: normalizaci√≥n y reescalado Lanczos aplicados!\")\n",
    "print(f\"Im√°genes DUD procesadas guardadas en: {processed_dud_npy_dir}\")\n",
    "print(f\"Im√°genes ACS procesadas guardadas en: {processed_acs_npy_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4b8a80-e6cc-459b-82ce-4c236edc0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PASO 2: Aplicando Padding/Reescalado Lanczos a datos .npy normalizados ---\n",
      "Directorio de salida '../data/log/normalized/pading608\\dud' creado.\n",
      "\n",
      "Procesando im√°genes .npy en '../data/log/normalized/dud' a 152x152 (padding/reescalado Lanczos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [00:17<00:00, 927.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de salida '../data/log/normalized/pading608\\acs' creado.\n",
      "\n",
      "Procesando im√°genes .npy en '../data/log/normalized/acs' a 608x608 (padding/reescalado Lanczos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [00:45<00:00, 351.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ ¬°Procesamiento completo: normalizaci√≥n y reescalado Lanczos aplicados!\n",
      "Im√°genes DUD procesadas guardadas en: ../data/log/normalized/pading608\\dud\n",
      "Im√°genes ACS procesadas guardadas en: ../data/log/normalized/pading608\\acs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Solo pading\n",
    "import os\n",
    "import numpy as np\n",
    "#from astropy.io import fits # Todav√≠a se usa si quieres el header, pero no para leer la imagen\n",
    "from scipy.ndimage import zoom\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_normalized_npy(input_npy_dir, output_npy_dir, target_resolution):\n",
    "    \"\"\"\n",
    "    Aplica padding a im√°genes .npy peque√±as y reescalado Lanczos a im√°genes .npy grandes\n",
    "    para alcanzar una resoluci√≥n cuadrada objetivo. Guarda las im√°genes resultantes\n",
    "    como archivos .npy.\n",
    "\n",
    "    Args:\n",
    "        input_npy_dir (str): Directorio de entrada con las im√°genes .npy normalizadas.\n",
    "        output_npy_dir (str): Directorio donde se guardar√°n las im√°genes .npy procesadas.\n",
    "        target_resolution (int): La resoluci√≥n cuadrada deseada (ej. 21 para 21x21).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_npy_dir):\n",
    "        print(f\"Error: El directorio de entrada '{input_npy_dir}' no existe.\")\n",
    "        return\n",
    "    if not os.path.exists(output_npy_dir):\n",
    "        os.makedirs(output_npy_dir)\n",
    "        print(f\"Directorio de salida '{output_npy_dir}' creado.\")\n",
    "\n",
    "    npy_files = [f for f in os.listdir(input_npy_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "    if not npy_files:\n",
    "        print(f\"No se encontraron archivos .npy en '{input_npy_dir}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcesando im√°genes .npy en '{input_npy_dir}' a {target_resolution}x{target_resolution} (padding/reescalado Lanczos)...\")\n",
    "\n",
    "    for filename in tqdm(npy_files):\n",
    "        input_path = os.path.join(input_npy_dir, filename)\n",
    "        output_path = os.path.join(output_npy_dir, filename) # Se mantiene el mismo nombre\n",
    "\n",
    "        try:\n",
    "            original_data = np.load(input_path)\n",
    "            original_height, original_width = original_data.shape\n",
    "            original_dtype = original_data.dtype # Usamos el dtype original, que ser√° float32\n",
    "\n",
    "            processed_data = np.zeros((target_resolution, target_resolution), dtype=original_dtype)\n",
    "\n",
    "            # --- L√≥gica de procesamiento ---\n",
    "            if original_width < target_resolution or original_height < target_resolution:\n",
    "                # Aplicar padding para im√°genes peque√±as\n",
    "                start_y = (target_resolution - original_height) // 2\n",
    "                start_x = (target_resolution - original_width) // 2\n",
    "                \n",
    "                processed_data[start_y : start_y + original_height,\n",
    "                               start_x : start_x + original_width] = original_data\n",
    "            else:\n",
    "                # Reescalar Lanczos para im√°genes grandes o de tama√±o similar\n",
    "                zoom_factor_h = target_resolution / original_height\n",
    "                zoom_factor_w = target_resolution / original_width\n",
    "\n",
    "                # No necesitamos convertir a float32 aqu√≠ porque ya est√°n normalizadas (float32)\n",
    "                zoomed_data = zoom(original_data, (zoom_factor_h, zoom_factor_w), order=5) \n",
    "                \n",
    "                # Aseguramos que la imagen reescalada tenga exactamente la resoluci√≥n objetivo\n",
    "                current_h, current_w = zoomed_data.shape\n",
    "                \n",
    "                if current_h != target_resolution or current_w != target_resolution:\n",
    "                    final_zoomed_data = np.zeros((target_resolution, target_resolution), dtype=original_dtype)\n",
    "                    \n",
    "                    insert_h = min(target_resolution, current_h)\n",
    "                    insert_w = min(target_resolution, current_w)\n",
    "\n",
    "                    start_y_zoom = (target_resolution - insert_h) // 2\n",
    "                    start_x_zoom = (target_resolution - insert_w) // 2\n",
    "\n",
    "                    src_start_y = (current_h - insert_h) // 2\n",
    "                    src_start_x = (current_w - insert_w) // 2\n",
    "\n",
    "                    final_zoomed_data[start_y_zoom : start_y_zoom + insert_h,\n",
    "                                      start_x_zoom : start_x_zoom + insert_w] = \\\n",
    "                                      zoomed_data[src_start_y : src_start_y + insert_h,\n",
    "                                                  src_start_x : src_start_x + insert_w]\n",
    "                    processed_data = final_zoomed_data.astype(original_dtype)\n",
    "                else:\n",
    "                    processed_data = zoomed_data.astype(original_dtype)\n",
    "            \n",
    "            # Guardar el archivo .npy procesado\n",
    "            np.save(output_path, processed_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {filename} (reescalado Lanczos): {e}\")\n",
    "\n",
    "# Directorios de salida para los datos normalizados (.npy)\n",
    "normalized_base_dir = \"../data/log/normalized/\"\n",
    "dud_normalized_dir = os.path.join(normalized_base_dir, \"dud\")\n",
    "acs_normalized_dir = os.path.join(normalized_base_dir, \"acs\")\n",
    "\n",
    "# Define los directorios de salida para las im√°genes .npy procesadas\n",
    "# (despu√©s de normalizaci√≥n y reescalado/padding)\n",
    "processed_dud_npy_dir = os.path.join(normalized_base_dir,\"pading608\", \"dud\")\n",
    "processed_acs_npy_dir = os.path.join(normalized_base_dir,\"pading608\", \"acs\")\n",
    "\n",
    "# Definir resoluciones objetivo (las mismas que antes)\n",
    "target_dud_resolution = 152\n",
    "target_acs_resolution = 608\n",
    "\n",
    "print(\"\\n--- PASO 2: Aplicando Padding/Reescalado Lanczos a datos .npy normalizados ---\")\n",
    "\n",
    "# Procesar las im√°genes DUD normalizadas\n",
    "process_normalized_npy(dud_normalized_dir, processed_dud_npy_dir, target_dud_resolution)\n",
    "\n",
    "# Procesar las im√°genes ACS normalizadas\n",
    "process_normalized_npy(acs_normalized_dir, processed_acs_npy_dir, target_acs_resolution)\n",
    "\n",
    "print(\"\\nüéâ ¬°Procesamiento completo: normalizaci√≥n y reescalado Lanczos aplicados!\")\n",
    "print(f\"Im√°genes DUD procesadas guardadas en: {processed_dud_npy_dir}\")\n",
    "print(f\"Im√°genes ACS procesadas guardadas en: {processed_acs_npy_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573f09fb-4214-4695-b0cf-56de4a8df611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aumento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6814225d-c1db-4ede-8860-97ece43d26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rotaci√≥n y flip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8016d4-e2cc-4375-b620-907b9b556199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generando aumentos de datos y copiando originales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [26:10<00:00, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Aumento de datos completado.\n",
      "üîπ Pares originales: 16114\n",
      "üî∏ Datos aumentados: 112798\n",
      "üì¶ Total final de archivos: 128912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directorios\n",
    "input_dud_dir = \"../data/log/normalized/pading608/dud/\"\n",
    "input_acs_dir = \"../data/log/normalized/pading608/acs/\"\n",
    "\n",
    "output_dud_dir = \"../data/log/AugmentationLow608/dud/\"\n",
    "output_acs_dir = \"../data/log/AugmentationLow608/acs/\"\n",
    "\n",
    "os.makedirs(output_dud_dir, exist_ok=True)\n",
    "os.makedirs(output_acs_dir, exist_ok=True)\n",
    "\n",
    "# Aumentos (funci√≥n, sufijo)\n",
    "augmentations = [\n",
    "    (lambda x: np.rot90(x, 1), \"rot90\"),\n",
    "    (lambda x: np.rot90(x, 2), \"rot180\"),\n",
    "    (lambda x: np.rot90(x, 3), \"rot270\"),\n",
    "    (lambda x: np.flipud(x), \"flipud\"),\n",
    "    (lambda x: np.fliplr(x), \"fliplr\"),\n",
    "    (lambda x: np.flipud(np.rot90(x, 1)), \"flipud_rot90\"),\n",
    "    (lambda x: np.fliplr(np.rot90(x, 1)), \"fliplr_rot90\"),\n",
    "]\n",
    "\n",
    "original_count = 0\n",
    "augmented_count = 0\n",
    "\n",
    "print(\"üîÑ Generando aumentos de datos y copiando originales...\")\n",
    "\n",
    "# Buscar pares v√°lidos\n",
    "dud_files = [f for f in os.listdir(input_dud_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "for dud_file in tqdm(dud_files):\n",
    "    base_name = dud_file.replace(\".npy\", \"\")\n",
    "    acs_file = base_name + \".npy\"\n",
    "\n",
    "    dud_path = os.path.join(input_dud_dir, dud_file)\n",
    "    acs_path = os.path.join(input_acs_dir, acs_file)\n",
    "\n",
    "    if not os.path.exists(acs_path):\n",
    "        print(f\"‚ö†Ô∏è Falta el archivo ACS para {dud_file}\")\n",
    "        continue\n",
    "\n",
    "    # Cargar datos\n",
    "    dud_data = np.load(dud_path)\n",
    "    acs_data = np.load(acs_path)\n",
    "\n",
    "    # Guardar copia original\n",
    "    np.save(os.path.join(output_dud_dir, f\"{base_name}.npy\"), dud_data)\n",
    "    np.save(os.path.join(output_acs_dir, f\"{base_name}.npy\"), acs_data)\n",
    "    original_count += 1\n",
    "\n",
    "    # Aumentos\n",
    "    for i, (aug_fn, suffix) in enumerate(augmentations, start=1):\n",
    "        aug_dud = aug_fn(dud_data)\n",
    "        aug_acs = aug_fn(acs_data)\n",
    "\n",
    "        aug_dud_path = os.path.join(output_dud_dir, f\"{base_name}_aug{i}_{suffix}.npy\")\n",
    "        aug_acs_path = os.path.join(output_acs_dir, f\"{base_name}_aug{i}_{suffix}.npy\")\n",
    "\n",
    "        np.save(aug_dud_path, aug_dud)\n",
    "        np.save(aug_acs_path, aug_acs)\n",
    "        augmented_count += 1\n",
    "\n",
    "# Estad√≠sticas finales\n",
    "total = original_count + augmented_count\n",
    "print(\"\\n‚úÖ Aumento de datos completado.\")\n",
    "print(f\"üîπ Pares originales: {original_count}\")\n",
    "print(f\"üî∏ Datos aumentados: {augmented_count}\")\n",
    "print(f\"üì¶ Total final de archivos: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418de219-446a-4e46-8419-69e87e8a9bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen cargada de ../data/log/AugmentationLow608/dud/n10_0_87915.npy\n",
      "Resoluci√≥n: (152, 152)\n",
      "Imagen cargada de ../data/log/AugmentationLow608/acs/n10_0_87915.npy\n",
      "Resoluci√≥n: (608, 608)\n"
     ]
    }
   ],
   "source": [
    "# Procesado para adaptar los datos a la SRGAN\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Directorios con los archivos .npy\n",
    "output_dud_dir = \"../data/log/AugmentationLow608/dud/\"\n",
    "output_acs_dir = \"../data/log/AugmentationLow608/acs/\"\n",
    "\n",
    "# Funci√≥n para cargar y mostrar resoluci√≥n de un archivo .npy\n",
    "def check_resolution(dir_path):\n",
    "    # Lista de archivos .npy\n",
    "    npy_files = [f for f in os.listdir(dir_path) if f.endswith(\".npy\")]\n",
    "    if not npy_files:\n",
    "        print(f\"No hay archivos .npy en {dir_path}\")\n",
    "        return\n",
    "\n",
    "    # Seleccionamos el primero\n",
    "    npy_path = os.path.join(dir_path, npy_files[0])\n",
    "    img = np.load(npy_path)\n",
    "\n",
    "    print(f\"Imagen cargada de {npy_path}\")\n",
    "    print(f\"Resoluci√≥n: {img.shape}\")  # Por ejemplo: (152, 152, 3)\n",
    "\n",
    "# Mostrar resoluci√≥n de una imagen en cada carpeta\n",
    "check_resolution(output_dud_dir)\n",
    "check_resolution(output_acs_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5293bb-c600-4b7c-b0ea-06a59f1aaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zoom y deslizamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774619d6-bb4e-452d-907a-d3aa5799f534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Generando aumentos por zoom con deslizamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñç                                                                          | 825/128912 [01:33<5:27:12,  6.52it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directorios de entrada\n",
    "input_dud_dir = \"../data/log/Augmentation/dud/\"\n",
    "input_acs_dir = \"../data/log/Augmentation/acs/\"\n",
    "\n",
    "# Directorios de salida\n",
    "output_dud_dir = \"../data/log/AugZum/dud/\"\n",
    "output_acs_dir = \"../data/log/AugZum/acs/\"\n",
    "os.makedirs(output_dud_dir, exist_ok=True)\n",
    "os.makedirs(output_acs_dir, exist_ok=True)\n",
    "\n",
    "# Par√°metros de zoom (2 p√≠xeles de margen)\n",
    "crop_margin = 2\n",
    "original_count = 0\n",
    "augmented_count = 0\n",
    "\n",
    "print(\"üîç Generando aumentos por zoom con deslizamiento...\")\n",
    "\n",
    "dud_files = [f for f in os.listdir(input_dud_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "for file in tqdm(dud_files):\n",
    "    base_name = file.replace(\".npy\", \"\")\n",
    "    dud_path = os.path.join(input_dud_dir, file)\n",
    "    acs_path = os.path.join(input_acs_dir, file)\n",
    "\n",
    "    if not os.path.exists(acs_path):\n",
    "        print(f\"‚ö†Ô∏è Falta el archivo ACS para {file}\")\n",
    "        continue\n",
    "\n",
    "    # Cargar im√°genes\n",
    "    dud = np.load(dud_path)\n",
    "    acs = np.load(acs_path)\n",
    "\n",
    "    h_dud, w_dud = dud.shape\n",
    "    h_acs, w_acs = acs.shape\n",
    "\n",
    "    # Guardar copia original\n",
    "    np.save(os.path.join(output_dud_dir, file), dud)\n",
    "    np.save(os.path.join(output_acs_dir, file), acs)\n",
    "    original_count += 1\n",
    "\n",
    "    # 4 crops con desplazamiento de 2 p√≠xeles\n",
    "    for idx, (dy, dx) in enumerate([(0, 0), (0, crop_margin), (crop_margin, 0), (crop_margin, crop_margin)], start=1):\n",
    "        dud_crop = dud[dy:h_dud - crop_margin + dy, dx:w_dud - crop_margin + dx]\n",
    "        acs_crop = acs[dy:h_acs - crop_margin + dy, dx:w_acs - crop_margin + dx]\n",
    "\n",
    "        dud_out = os.path.join(output_dud_dir, f\"{base_name}_zoom{idx}.npy\")\n",
    "        acs_out = os.path.join(output_acs_dir, f\"{base_name}_zoom{idx}.npy\")\n",
    "\n",
    "        np.save(dud_out, dud_crop)\n",
    "        np.save(acs_out, acs_crop)\n",
    "        augmented_count += 1\n",
    "\n",
    "# Resumen\n",
    "total = original_count + augmented_count\n",
    "print(\"\\n‚úÖ Zoom con deslizamiento completado.\")\n",
    "print(f\"üîπ Originales procesados: {original_count}\")\n",
    "print(f\"üî∏ Zooms generados: {augmented_count}\")\n",
    "print(f\"üì¶ Total final: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c207e1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16): 22784 im√°genes\n",
      "(13, 13): 29624 im√°genes\n",
      "(26, 26): 6344 im√°genes\n",
      "(10, 11): 7076 im√°genes\n",
      "(14, 14): 27472 im√°genes\n",
      "(17, 16): 5376 im√°genes\n",
      "(31, 31): 3768 im√°genes\n",
      "(10, 10): 25392 im√°genes\n",
      "(22, 21): 3544 im√°genes\n",
      "(71, 72): 80 im√°genes\n",
      "(7, 8): 3216 im√°genes\n",
      "(15, 15): 22736 im√°genes\n",
      "(11, 11): 25160 im√°genes\n",
      "(8, 8): 13896 im√°genes\n",
      "(48, 48): 792 im√°genes\n",
      "(14, 13): 7008 im√°genes\n",
      "(30, 30): 4032 im√°genes\n",
      "(9, 9): 23664 im√°genes\n",
      "(11, 12): 6520 im√°genes\n",
      "(6, 7): 1264 im√°genes\n",
      "(18, 18): 14280 im√°genes\n",
      "(12, 13): 5560 im√°genes\n",
      "(31, 32): 820 im√°genes\n",
      "(22, 22): 14496 im√°genes\n",
      "(9, 8): 4672 im√°genes\n",
      "(19, 20): 3828 im√°genes\n",
      "(17, 17): 21712 im√°genes\n",
      "(18, 17): 4120 im√°genes\n",
      "(13, 14): 7008 im√°genes\n",
      "(14, 15): 6328 im√°genes\n",
      "(19, 18): 4580 im√°genes\n",
      "(12, 12): 31968 im√°genes\n",
      "(20, 20): 12968 im√°genes\n",
      "(20, 19): 3828 im√°genes\n",
      "(16, 17): 5376 im√°genes\n",
      "(8, 9): 4672 im√°genes\n",
      "(7, 7): 11304 im√°genes\n",
      "(16, 15): 5292 im√°genes\n",
      "(28, 28): 5168 im√°genes\n",
      "(25, 25): 7336 im√°genes\n",
      "(35, 35): 1968 im√°genes\n",
      "(41, 41): 1728 im√°genes\n",
      "(21, 21): 10576 im√°genes\n",
      "(21, 20): 3836 im√°genes\n",
      "(7, 6): 1264 im√°genes\n",
      "(10, 9): 5584 im√°genes\n",
      "(35, 36): 788 im√°genes\n",
      "(38, 38): 2048 im√°genes\n",
      "(38, 37): 476 im√°genes\n",
      "(37, 37): 2264 im√°genes\n",
      "(19, 19): 18536 im√°genes\n",
      "(24, 25): 1972 im√°genes\n",
      "(32, 32): 4152 im√°genes\n",
      "(149, 150): 16 im√°genes\n",
      "(12, 11): 6520 im√°genes\n",
      "(6, 6): 3680 im√°genes\n",
      "(15, 14): 6328 im√°genes\n",
      "(97, 97): 40 im√°genes\n",
      "(41, 42): 280 im√°genes\n",
      "(64, 64): 312 im√°genes\n",
      "(5, 5): 968 im√°genes\n",
      "(8, 7): 3216 im√°genes\n",
      "(44, 45): 284 im√°genes\n",
      "(18, 19): 4580 im√°genes\n",
      "(76, 76): 312 im√°genes\n",
      "(15, 16): 5292 im√°genes\n",
      "(29, 29): 5912 im√°genes\n",
      "(25, 26): 1976 im√°genes\n",
      "(20, 21): 3836 im√°genes\n",
      "(23, 24): 2392 im√°genes\n",
      "(44, 44): 1016 im√°genes\n",
      "(43, 44): 280 im√°genes\n",
      "(36, 36): 2520 im√°genes\n",
      "(39, 39): 1624 im√°genes\n",
      "(82, 81): 72 im√°genes\n",
      "(25, 24): 1972 im√°genes\n",
      "(54, 54): 672 im√°genes\n",
      "(40, 40): 2120 im√°genes\n",
      "(27, 28): 1060 im√°genes\n",
      "(9, 10): 5584 im√°genes\n",
      "(36, 37): 400 im√°genes\n",
      "(33, 32): 808 im√°genes\n",
      "(44, 43): 280 im√°genes\n",
      "(11, 10): 7076 im√°genes\n",
      "(24, 24): 9144 im√°genes\n",
      "(70, 70): 304 im√°genes\n",
      "(43, 42): 256 im√°genes\n",
      "(41, 40): 392 im√°genes\n",
      "(28, 29): 1168 im√°genes\n",
      "(46, 46): 1032 im√°genes\n",
      "(103, 103): 16 im√°genes\n",
      "(32, 33): 808 im√°genes\n",
      "(13, 12): 5560 im√°genes\n",
      "(5, 6): 856 im√°genes\n",
      "(24, 23): 2392 im√°genes\n",
      "(31, 30): 1024 im√°genes\n",
      "(47, 48): 280 im√°genes\n",
      "(23, 23): 8064 im√°genes\n",
      "(33, 33): 2512 im√°genes\n",
      "(60, 59): 60 im√°genes\n",
      "(30, 31): 1024 im√°genes\n",
      "(81, 81): 280 im√°genes\n",
      "(40, 39): 304 im√°genes\n",
      "(17, 18): 4120 im√°genes\n",
      "(74, 73): 128 im√°genes\n",
      "(33, 34): 816 im√°genes\n",
      "(51, 52): 88 im√°genes\n",
      "(49, 48): 212 im√°genes\n",
      "(34, 34): 3248 im√°genes\n",
      "(42, 43): 256 im√°genes\n",
      "(49, 50): 220 im√°genes\n",
      "(21, 22): 3544 im√°genes\n",
      "(72, 73): 56 im√°genes\n",
      "(93, 93): 72 im√°genes\n",
      "(34, 35): 792 im√°genes\n",
      "(27, 26): 1488 im√°genes\n",
      "(27, 27): 6784 im√°genes\n",
      "(32, 31): 820 im√°genes\n",
      "(55, 56): 148 im√°genes\n",
      "(43, 43): 1096 im√°genes\n",
      "(26, 25): 1976 im√°genes\n",
      "(22, 23): 2224 im√°genes\n",
      "(34, 33): 816 im√°genes\n",
      "(62, 62): 456 im√°genes\n",
      "(42, 42): 1808 im√°genes\n",
      "(45, 45): 1432 im√°genes\n",
      "(101, 101): 80 im√°genes\n",
      "(60, 60): 472 im√°genes\n",
      "(4, 4): 264 im√°genes\n",
      "(51, 51): 600 im√°genes\n",
      "(48, 47): 280 im√°genes\n",
      "(40, 41): 392 im√°genes\n",
      "(120, 121): 16 im√°genes\n",
      "(72, 71): 80 im√°genes\n",
      "(69, 69): 448 im√°genes\n",
      "(30, 29): 888 im√°genes\n",
      "(47, 47): 984 im√°genes\n",
      "(74, 74): 264 im√°genes\n",
      "(78, 77): 36 im√°genes\n",
      "(50, 49): 220 im√°genes\n",
      "(3, 3): 168 im√°genes\n",
      "(23, 22): 2224 im√°genes\n",
      "(28, 27): 1060 im√°genes\n",
      "(59, 60): 60 im√°genes\n",
      "(47, 46): 268 im√°genes\n",
      "(69, 70): 80 im√°genes\n",
      "(84, 84): 48 im√°genes\n",
      "(67, 67): 352 im√°genes\n",
      "(53, 53): 336 im√°genes\n",
      "(59, 59): 408 im√°genes\n",
      "(36, 35): 788 im√°genes\n",
      "(52, 51): 88 im√°genes\n",
      "(29, 28): 1168 im√°genes\n",
      "(71, 70): 44 im√°genes\n",
      "(58, 57): 140 im√°genes\n",
      "(26, 27): 1488 im√°genes\n",
      "(78, 78): 160 im√°genes\n",
      "(6, 5): 856 im√°genes\n",
      "(35, 34): 792 im√°genes\n",
      "(39, 38): 348 im√°genes\n",
      "(50, 50): 720 im√°genes\n",
      "(110, 109): 16 im√°genes\n",
      "(117, 117): 48 im√°genes\n",
      "(53, 52): 184 im√°genes\n",
      "(46, 47): 268 im√°genes\n",
      "(65, 64): 72 im√°genes\n",
      "(65, 65): 416 im√°genes\n",
      "(58, 58): 288 im√°genes\n",
      "(39, 40): 304 im√°genes\n",
      "(73, 74): 128 im√°genes\n",
      "(79, 80): 40 im√°genes\n",
      "(75, 75): 328 im√°genes\n",
      "(79, 79): 304 im√°genes\n",
      "(45, 44): 284 im√°genes\n",
      "(123, 122): 20 im√°genes\n",
      "(52, 52): 784 im√°genes\n",
      "(4, 3): 96 im√°genes\n",
      "(71, 71): 256 im√°genes\n",
      "(112, 111): 4 im√°genes\n",
      "(77, 77): 400 im√°genes\n",
      "(37, 38): 476 im√°genes\n",
      "(49, 49): 776 im√°genes\n",
      "(37, 36): 400 im√°genes\n",
      "(81, 82): 72 im√°genes\n",
      "(3, 2): 68 im√°genes\n",
      "(51, 50): 136 im√°genes\n",
      "(45, 46): 216 im√°genes\n",
      "(55, 54): 136 im√°genes\n",
      "(94, 94): 88 im√°genes\n",
      "(74, 75): 44 im√°genes\n",
      "(38, 39): 348 im√°genes\n",
      "(42, 41): 280 im√°genes\n",
      "(72, 72): 224 im√°genes\n",
      "(67, 66): 60 im√°genes\n",
      "(85, 84): 52 im√°genes\n",
      "(80, 80): 152 im√°genes\n",
      "(60, 61): 100 im√°genes\n",
      "(67, 68): 72 im√°genes\n",
      "(83, 83): 120 im√°genes\n",
      "(68, 69): 60 im√°genes\n",
      "(89, 89): 96 im√°genes\n",
      "(107, 107): 40 im√°genes\n",
      "(73, 73): 72 im√°genes\n",
      "(55, 55): 344 im√°genes\n",
      "(57, 57): 424 im√°genes\n",
      "(119, 120): 4 im√°genes\n",
      "(76, 75): 44 im√°genes\n",
      "(82, 82): 96 im√°genes\n",
      "(54, 53): 156 im√°genes\n",
      "(29, 30): 888 im√°genes\n",
      "(68, 68): 248 im√°genes\n",
      "(96, 96): 112 im√°genes\n",
      "(73, 72): 56 im√°genes\n",
      "(66, 66): 280 im√°genes\n",
      "(120, 119): 4 im√°genes\n",
      "(98, 98): 88 im√°genes\n",
      "(4, 5): 208 im√°genes\n",
      "(95, 94): 36 im√°genes\n",
      "(56, 56): 384 im√°genes\n",
      "(77, 78): 36 im√°genes\n",
      "(92, 92): 96 im√°genes\n",
      "(84, 83): 32 im√°genes\n",
      "(50, 51): 136 im√°genes\n",
      "(61, 62): 72 im√°genes\n",
      "(2, 3): 68 im√°genes\n",
      "(63, 62): 112 im√°genes\n",
      "(103, 104): 32 im√°genes\n",
      "(87, 87): 136 im√°genes\n",
      "(87, 88): 20 im√°genes\n",
      "(61, 60): 100 im√°genes\n",
      "(3, 4): 96 im√°genes\n",
      "(69, 68): 60 im√°genes\n",
      "(122, 123): 20 im√°genes\n",
      "(5, 4): 208 im√°genes\n",
      "(63, 63): 176 im√°genes\n",
      "(83, 84): 32 im√°genes\n",
      "(48, 49): 212 im√°genes\n",
      "(86, 85): 20 im√°genes\n",
      "(58, 59): 172 im√°genes\n",
      "(46, 45): 216 im√°genes\n",
      "(56, 57): 136 im√°genes\n",
      "(94, 95): 36 im√°genes\n",
      "(66, 67): 60 im√°genes\n",
      "(114, 114): 40 im√°genes\n",
      "(52, 53): 184 im√°genes\n",
      "(1, 1): 32 im√°genes\n",
      "(53, 54): 156 im√°genes\n",
      "(64, 65): 72 im√°genes\n",
      "(91, 91): 48 im√°genes\n",
      "(104, 104): 40 im√°genes\n",
      "(68, 67): 72 im√°genes\n",
      "(61, 61): 272 im√°genes\n",
      "(115, 115): 64 im√°genes\n",
      "(112, 112): 40 im√°genes\n",
      "(127, 126): 4 im√°genes\n",
      "(57, 58): 140 im√°genes\n",
      "(70, 71): 44 im√°genes\n",
      "(62, 63): 112 im√°genes\n",
      "(89, 90): 20 im√°genes\n",
      "(95, 95): 48 im√°genes\n",
      "(138, 138): 40 im√°genes\n",
      "(2, 2): 32 im√°genes\n",
      "(110, 110): 48 im√°genes\n",
      "(56, 55): 148 im√°genes\n",
      "(76, 77): 8 im√°genes\n",
      "(54, 55): 136 im√°genes\n",
      "(59, 58): 172 im√°genes\n",
      "(116, 115): 16 im√°genes\n",
      "(136, 136): 32 im√°genes\n",
      "(129, 129): 32 im√°genes\n",
      "(88, 88): 48 im√°genes\n",
      "(95, 96): 16 im√°genes\n",
      "(91, 92): 36 im√°genes\n",
      "(92, 91): 36 im√°genes\n",
      "(93, 92): 20 im√°genes\n",
      "(132, 132): 32 im√°genes\n",
      "(150, 150): 32 im√°genes\n",
      "(57, 56): 136 im√°genes\n",
      "(97, 96): 8 im√°genes\n",
      "(64, 63): 64 im√°genes\n",
      "(125, 126): 16 im√°genes\n",
      "(66, 65): 44 im√°genes\n",
      "(105, 105): 32 im√°genes\n",
      "(99, 99): 72 im√°genes\n",
      "(102, 102): 40 im√°genes\n",
      "(70, 69): 80 im√°genes\n",
      "(108, 108): 64 im√°genes\n",
      "(90, 91): 20 im√°genes\n",
      "(107, 106): 16 im√°genes\n",
      "(86, 87): 28 im√°genes\n",
      "(62, 61): 72 im√°genes\n",
      "(105, 106): 8 im√°genes\n",
      "(133, 133): 16 im√°genes\n",
      "(86, 86): 72 im√°genes\n",
      "(65, 66): 44 im√°genes\n",
      "(85, 85): 48 im√°genes\n",
      "(89, 88): 20 im√°genes\n",
      "(80, 79): 40 im√°genes\n",
      "(87, 86): 28 im√°genes\n",
      "(104, 103): 32 im√°genes\n",
      "(90, 89): 20 im√°genes\n",
      "(1, 0): 16 im√°genes\n",
      "(88, 89): 20 im√°genes\n",
      "(152, 151): 4 im√°genes\n",
      "(125, 124): 20 im√°genes\n",
      "(115, 116): 16 im√°genes\n",
      "(85, 86): 20 im√°genes\n",
      "(99, 98): 16 im√°genes\n",
      "(100, 100): 48 im√°genes\n",
      "(118, 117): 20 im√°genes\n",
      "(75, 74): 44 im√°genes\n",
      "(63, 64): 64 im√°genes\n",
      "(101, 100): 4 im√°genes\n",
      "(91, 90): 20 im√°genes\n",
      "(106, 107): 16 im√°genes\n",
      "(131, 131): 72 im√°genes\n",
      "(109, 109): 8 im√°genes\n",
      "(84, 85): 52 im√°genes\n",
      "(90, 90): 8 im√°genes\n",
      "(79, 78): 16 im√°genes\n",
      "(152, 152): 8 im√°genes\n",
      "(96, 97): 8 im√°genes\n",
      "(116, 116): 8 im√°genes\n",
      "(134, 134): 8 im√°genes\n",
      "(151, 152): 4 im√°genes\n",
      "(94, 93): 8 im√°genes\n",
      "(77, 76): 8 im√°genes\n",
      "(93, 94): 8 im√°genes\n",
      "(81, 80): 4 im√°genes\n",
      "(75, 76): 44 im√°genes\n",
      "(126, 127): 4 im√°genes\n",
      "(92, 93): 20 im√°genes\n",
      "(106, 106): 8 im√°genes\n",
      "(98, 97): 4 im√°genes\n",
      "(140, 140): 8 im√°genes\n",
      "(80, 81): 4 im√°genes\n",
      "(127, 128): 4 im√°genes\n",
      "(124, 125): 20 im√°genes\n",
      "(106, 105): 8 im√°genes\n",
      "(97, 98): 4 im√°genes\n",
      "(128, 127): 4 im√°genes\n",
      "(119, 119): 8 im√°genes\n",
      "(96, 95): 16 im√°genes\n",
      "(88, 87): 20 im√°genes\n",
      "(121, 120): 16 im√°genes\n",
      "(78, 79): 16 im√°genes\n",
      "(150, 149): 16 im√°genes\n",
      "(117, 118): 20 im√°genes\n",
      "(0, 1): 16 im√°genes\n",
      "(109, 108): 4 im√°genes\n",
      "(109, 110): 16 im√°genes\n",
      "(82, 83): 16 im√°genes\n",
      "(108, 109): 4 im√°genes\n",
      "(126, 125): 16 im√°genes\n",
      "(100, 101): 4 im√°genes\n",
      "(111, 112): 4 im√°genes\n",
      "(98, 99): 16 im√°genes\n",
      "(83, 82): 16 im√°genes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    " \n",
    "folder_path = \"../data/AugZum/dud\"  \n",
    " \n",
    "shapes = []\n",
    " \n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".npy\"):\n",
    "        path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            arr = np.load(path)\n",
    "            shapes.append(arr.shape)\n",
    "        except Exception as e:\n",
    "            print(f\"Error leyendo {file}: {e}\")\n",
    " \n",
    "# Mostrar cu√°ntas im√°genes hay de cada tama√±o\n",
    "conteo = Counter(shapes)\n",
    "for shape, count in conteo.items():\n",
    "    print(f\"{shape}: {count} im√°genes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023f3f3-5d0e-4e26-9fba-a69dbde7d83f",
   "metadata": {},
   "source": [
    "# Split en train, val y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76942b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dividiendo datos en conjuntos train/val/test...\n",
      "\n",
      "Clase dud:\n",
      "  Total: 128912 im√°genes\n",
      "  Train: 103129 im√°genes (80.0%)\n",
      "  Val:   12891 im√°genes (10.0%)\n",
      "  Test:  12892 im√°genes (10.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copiando dud a train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103129/103129 [02:45<00:00, 623.99it/s]\n",
      "Copiando dud a val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12891/12891 [00:18<00:00, 679.76it/s]\n",
      "Copiando dud a test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12892/12892 [00:19<00:00, 665.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clase acs:\n",
      "  Total: 128912 im√°genes\n",
      "  Train: 103129 im√°genes (80.0%)\n",
      "  Val:   12891 im√°genes (10.0%)\n",
      "  Test:  12892 im√°genes (10.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copiando acs a train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103129/103129 [09:36<00:00, 179.03it/s]\n",
      "Copiando acs a val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12891/12891 [01:09<00:00, 186.65it/s]\n",
      "Copiando acs a test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12892/12892 [01:09<00:00, 185.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ ¬°Divisi√≥n de datos completada!\n",
      "Conjunto train: ../data/log/AugmentationLow608/split_data\\train\n",
      "Conjunto val:   ../data/log/AugmentationLow608/split_data\\val\n",
      "Conjunto test:  ../data/log/AugmentationLow608/split_data\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_data_into_sets(input_dirs, output_base_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Divide los datos en conjuntos de entrenamiento, validaci√≥n y prueba, manteniendo la estructura de directorios.\n",
    "    \n",
    "    Args:\n",
    "        input_dirs (dict): Diccionario con las rutas de entrada para cada clase (ej. {'dud': path, 'acs': path})\n",
    "        output_base_dir (str): Directorio base donde se crear√°n los subdirectorios train/val/test\n",
    "        train_ratio (float): Proporci√≥n para el conjunto de entrenamiento (por defecto 0.7)\n",
    "        val_ratio (float): Proporci√≥n para el conjunto de validaci√≥n (por defecto 0.15)\n",
    "        test_ratio (float): Proporci√≥n para el conjunto de prueba (por defecto 0.15)\n",
    "        random_state (int): Semilla para reproducibilidad (por defecto 42)\n",
    "    \"\"\"\n",
    "    # Verificar que los ratios sumen 1\n",
    "    assert abs((train_ratio + val_ratio + test_ratio) - 1.0) < 1e-6, \"Los ratios deben sumar 1\"\n",
    "    \n",
    "    # Crear directorios de salida\n",
    "    train_dir = os.path.join(output_base_dir, 'train')\n",
    "    val_dir = os.path.join(output_base_dir, 'val')\n",
    "    test_dir = os.path.join(output_base_dir, 'test')\n",
    "    \n",
    "    for dir_path in [train_dir, val_dir, test_dir]:\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "            for class_name in input_dirs.keys():\n",
    "                os.makedirs(os.path.join(dir_path, class_name))\n",
    "    \n",
    "    print(\"\\nDividiendo datos en conjuntos train/val/test...\")\n",
    "    \n",
    "    for class_name, input_dir in input_dirs.items():\n",
    "        # Obtener lista de archivos\n",
    "        files = [f for f in os.listdir(input_dir) if f.endswith('.npy')]\n",
    "        files.sort()  # Para reproducibilidad\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"Advertencia: No se encontraron archivos .npy en {input_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Dividir en train y temp (val + test)\n",
    "        train_files, temp_files = train_test_split(\n",
    "            files, \n",
    "            test_size=(1 - train_ratio), \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Dividir temp en val y test\n",
    "        val_test_ratio = val_ratio / (val_ratio + test_ratio)\n",
    "        val_files, test_files = train_test_split(\n",
    "            temp_files, \n",
    "            test_size=(1 - val_test_ratio), \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nClase {class_name}:\")\n",
    "        print(f\"  Total: {len(files)} im√°genes\")\n",
    "        print(f\"  Train: {len(train_files)} im√°genes ({len(train_files)/len(files):.1%})\")\n",
    "        print(f\"  Val:   {len(val_files)} im√°genes ({len(val_files)/len(files):.1%})\")\n",
    "        print(f\"  Test:  {len(test_files)} im√°genes ({len(test_files)/len(files):.1%})\")\n",
    "        \n",
    "        # Funci√≥n para copiar archivos\n",
    "        def copy_files(files, dest_dir):\n",
    "            for file in tqdm(files, desc=f\"Copiando {class_name} a {os.path.basename(dest_dir)}\"):\n",
    "                src_path = os.path.join(input_dir, file)\n",
    "                dest_path = os.path.join(dest_dir, class_name, file)\n",
    "                shutil.copy2(src_path, dest_path)\n",
    "        \n",
    "        # Copiar archivos a sus directorios correspondientes\n",
    "        copy_files(train_files, train_dir)\n",
    "        copy_files(val_files, val_dir)\n",
    "        copy_files(test_files, test_dir)\n",
    "\n",
    "# Directorios de entrada (los mismos que usaste en tu c√≥digo anterior)\n",
    "processed_dud_npy_dir = \"../data/log/AugmentationLow608/dud/\"\n",
    "processed_acs_npy_dir = \"../data/log/AugmentationLow608/acs/\"\n",
    "\n",
    "#processed_dud_npy_dir = os.path.join(normalized_base_dir, \"pading\", \"dud\")\n",
    "#processed_acs_npy_dir = os.path.join(normalized_base_dir, \"pading\", \"acs\")\n",
    "\n",
    "# Directorio de salida para los conjuntos divididos\n",
    "split_data_dir = os.path.join(\"../data/log/AugmentationLow608/\", \"split_data\")\n",
    "#split_data_dir = os.path.join(normalized_base_dir, \"split_data\")\n",
    "\n",
    "# Definir los directorios de entrada para cada clase\n",
    "input_dirs = {\n",
    "    'dud': processed_dud_npy_dir,\n",
    "    'acs': processed_acs_npy_dir\n",
    "}\n",
    "\n",
    "# Dividir los datos (70% train, 15% val, 15% test)\n",
    "split_data_into_sets(\n",
    "    input_dirs=input_dirs,\n",
    "    output_base_dir=split_data_dir,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.10,\n",
    "    test_ratio=0.10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ ¬°Divisi√≥n de datos completada!\")\n",
    "print(f\"Conjunto train: {os.path.join(split_data_dir, 'train')}\")\n",
    "print(f\"Conjunto val:   {os.path.join(split_data_dir, 'val')}\")\n",
    "print(f\"Conjunto test:  {os.path.join(split_data_dir, 'test')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4414a533-9f77-44a0-8526-17b76baa22e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Todas las parejas dud-acs est√°n correctamente emparejadas en todos los conjuntos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def check_matching_pairs(base_dir):\n",
    "    \"\"\"\n",
    "    Verifica que todos los archivos dud tengan su correspondiente archivo acs en el mismo conjunto\n",
    "    y viceversa.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): Directorio base que contiene las subcarpetas train/val/test\n",
    "    \"\"\"\n",
    "    sets = ['train', 'val', 'test']\n",
    "    problems_found = False\n",
    "    \n",
    "    for set_name in sets:\n",
    "        set_path = os.path.join(base_dir, set_name)\n",
    "        dud_dir = os.path.join(set_path, 'dud')\n",
    "        acs_dir = os.path.join(set_path, 'acs')\n",
    "        \n",
    "        # Obtener listas de archivos (sin la extensi√≥n .npy)\n",
    "        dud_files = set([f.replace('.npy', '') for f in os.listdir(dud_dir) if f.endswith('.npy')])\n",
    "        acs_files = set([f.replace('.npy', '') for f in os.listdir(acs_dir) if f.endswith('.npy')])\n",
    "        \n",
    "        # Encontrar diferencias\n",
    "        dud_without_acs = dud_files - acs_files\n",
    "        acs_without_dud = acs_files - dud_files\n",
    "        \n",
    "        if dud_without_acs:\n",
    "            print(f\"‚ö†Ô∏è En {set_name}: {len(dud_without_acs)} archivos dud sin pareja acs\")\n",
    "            print(f\"   Ejemplos: {list(dud_without_acs)[:5]}\")\n",
    "            problems_found = True\n",
    "            \n",
    "        if acs_without_dud:\n",
    "            print(f\"‚ö†Ô∏è En {set_name}: {len(acs_without_dud)} archivos acs sin pareja dud\")\n",
    "            print(f\"   Ejemplos: {list(acs_without_dud)[:5]}\")\n",
    "            problems_found = True\n",
    "    \n",
    "    if not problems_found:\n",
    "        print(\"‚úÖ Todas las parejas dud-acs est√°n correctamente emparejadas en todos los conjuntos\")\n",
    "\n",
    "# Directorio base donde est√°n train/val/test\n",
    "normalized_base_dir = \"../data/log/AugmentationLow608/\"\n",
    "split_data_dir = os.path.join(normalized_base_dir, \"split_data\")\n",
    "check_matching_pairs(split_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c69859dc-a049-476f-8c90-10aa6535e689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miguel\\AppData\\Local\\Temp\\ipykernel_20836\\2766978106.py:35: RuntimeWarning: invalid value encountered in cast\n",
      "  data = (data * 255).clip(0, 255).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error procesando ../data/log/AugmentationLow608/split_data\\train\\acs\\n25_250_62252_aug4_flipud.npy: [WinError 32] El proceso no tiene acceso al archivo porque est√° siendo utilizado por otro proceso: '../data/log/AugmentationLow608/split_data\\\\train\\\\acs\\\\n25_250_62252_aug4_flipud.npy'\n",
      "‚úÖ Proceso TERMINADO\n"
     ]
    }
   ],
   "source": [
    "# convierte los archivos .npy a im√°genes .png\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def convert_npy_to_png_and_cleanup(base_dir):\n",
    "    \"\"\"\n",
    "    Convierte archivos .npy a .png en cada carpeta (dud/acs) dentro de train/val/test\n",
    "    y elimina los .npy despu√©s de convertirlos.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): Directorio base que contiene las subcarpetas train/val/test\n",
    "    \"\"\"\n",
    "    sets = ['train', 'val', 'test']\n",
    "    subfolders = ['dud', 'acs']\n",
    "    \n",
    "    for set_name in sets:\n",
    "        for subfolder in subfolders:\n",
    "            folder_path = os.path.join(base_dir, set_name, subfolder)\n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"‚ùå Carpeta no encontrada: {folder_path}\")\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith('.npy'):\n",
    "                    npy_path = os.path.join(folder_path, file)\n",
    "                    png_filename = file.replace('.npy', '.png')\n",
    "                    png_path = os.path.join(folder_path, png_filename)\n",
    "                    \n",
    "                    try:\n",
    "                        data = np.load(npy_path)\n",
    "                        \n",
    "                        # Normalizar si est√° en [0,1]\n",
    "                        if data.dtype != np.uint8:\n",
    "                            data = (data * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "                        plt.imsave(png_path, data, cmap='gray' if data.ndim == 2 else None)\n",
    "                        os.remove(npy_path)\n",
    "                        #print(f\"‚úÖ Convertido y eliminado: {npy_path} -> {png_path}\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Error procesando {npy_path}: {e}\")\n",
    "\n",
    "# Uso\n",
    "normalized_base_dir = \"../data/log/AugmentationLow608/\"\n",
    "split_data_dir = os.path.join(normalized_base_dir, \"split_data\")\n",
    "convert_npy_to_png_and_cleanup(split_data_dir)\n",
    "print(\"‚úÖ Proceso TERMINADO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
