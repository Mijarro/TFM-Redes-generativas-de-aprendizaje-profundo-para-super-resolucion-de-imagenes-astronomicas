{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1af132f-3f24-42bd-944b-9b3459470437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparaci√≥n de los dos conjuntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b5ccba-d041-49ff-8c08-56e8b577bd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceso completado.\n",
      "Pares encontrados y copiados: 16114\n",
      "Archivos DUD sin par ACS: 5886\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Rutas base\n",
    "base_dud = \"../data/raw/dud\"\n",
    "base_acs = \"../data/raw/acs/\"\n",
    "output_dud = \"../data/process/dud/\"\n",
    "output_acs = \"../data/process/acs/\"\n",
    "\n",
    "# Asegurar que las carpetas de salida existan\n",
    "os.makedirs(output_dud, exist_ok=True)\n",
    "os.makedirs(output_acs, exist_ok=True)\n",
    "\n",
    "# Subcarpetas que se deben explorar\n",
    "subdirs = [\"n1\", \"n2\"]\n",
    "\n",
    "paired_count = 0\n",
    "missing_count = 0\n",
    "\n",
    "for subdir in subdirs:\n",
    "    dud_dir = os.path.join(base_dud, subdir)\n",
    "    acs_dir = os.path.join(base_acs, subdir)\n",
    "    \n",
    "    if not os.path.isdir(dud_dir) or not os.path.isdir(acs_dir):\n",
    "        print(f\"Advertencia: faltan carpetas en {subdir}\")\n",
    "        continue\n",
    "    \n",
    "    dud_files = [f for f in os.listdir(dud_dir) if f.endswith(\".fits\")]\n",
    "    \n",
    "    for dud_file in dud_files:\n",
    "        # Construir el nombre correspondiente en ACS\n",
    "        base_name = dud_file.replace(\".fits\", \"\")\n",
    "        acs_file = base_name + \"_ACS.fits\"\n",
    "\n",
    "        dud_path = os.path.join(dud_dir, dud_file)\n",
    "        acs_path = os.path.join(acs_dir, acs_file)\n",
    "\n",
    "        if os.path.exists(acs_path):\n",
    "            # Copiar ambos archivos a su nueva ubicaci√≥n\n",
    "            shutil.copy2(dud_path, os.path.join(output_dud, dud_file))\n",
    "            shutil.copy2(acs_path, os.path.join(output_acs, acs_file))\n",
    "            paired_count += 1\n",
    "        else:\n",
    "            missing_count += 1\n",
    "\n",
    "print(f\"\\nProceso completado.\")\n",
    "print(f\"Pares encontrados y copiados: {paired_count}\")\n",
    "print(f\"Archivos DUD sin par ACS: {missing_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bb55a2-4a70-411e-ae94-ab7a79e7c7b5",
   "metadata": {},
   "source": [
    "# Procesado y normalizacion de los datos para su uso por un modelo\n",
    "No se ha hecho, pero estaria bien:  (Si se ha hecho una normalizaci√≥n global)\n",
    "\n",
    "üìâ 3. Normalizaci√≥n estad√≠stica por imagen  \n",
    "\n",
    "Justificaci√≥n: La normalizaci√≥n min-max global es buena para mantener una escala uniforme, pero si hay alta variabilidad entre im√°genes, podr√≠as experimentar con:  \n",
    "- Normalizaci√≥n por imagen individual usando media y desviaci√≥n t√≠pica (z-score).  \n",
    "- O bien escalar entre 0 y 1 por imagen, especialmente √∫til si el rango din√°mico var√≠a mucho entre ejemplos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98a9fc6-8417-4612-b8e8-427ea1f8b220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando y guardando como .npy en carpetas separadas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [01:37<00:00, 165.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Total de pares procesados y guardados: 16114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directorios de entrada\n",
    "dud_dir = \"../data/process/dud/\"\n",
    "acs_dir = \"../data/process/acs/\"\n",
    "\n",
    "# Directorios de salida\n",
    "output_base = \"../data/normaliced/\"\n",
    "dud_output_dir = os.path.join(output_base, \"dud\")\n",
    "acs_output_dir = os.path.join(output_base, \"acs\")\n",
    "os.makedirs(dud_output_dir, exist_ok=True)\n",
    "os.makedirs(acs_output_dir, exist_ok=True)\n",
    "\n",
    "# Normalizaci√≥n min-max\n",
    "def normalize_dud(x):\n",
    "    return (x + 7.353157043457031) / (366.3216857910156 + 7.353157043457031)\n",
    "\n",
    "def normalize_acs(x):\n",
    "    return (x + 5.457014560699463) / (136.9979248046875 + 5.457014560699463)\n",
    "\n",
    "# Procesar todos los archivos\n",
    "dud_files = [f for f in os.listdir(dud_dir) if f.endswith(\".fits\")]\n",
    "paired_count = 0\n",
    "\n",
    "print(\"Normalizando y guardando como .npy en carpetas separadas...\")\n",
    "for dud_file in tqdm(dud_files):\n",
    "    base_name = dud_file.replace(\".fits\", \"\")\n",
    "    acs_file = base_name + \"_ACS.fits\"\n",
    "\n",
    "    dud_path = os.path.join(dud_dir, dud_file)\n",
    "    acs_path = os.path.join(acs_dir, acs_file)\n",
    "\n",
    "    if not os.path.exists(acs_path):\n",
    "        continue  # saltar si falta el par\n",
    "\n",
    "    try:\n",
    "        # Cargar y normalizar DUD\n",
    "        with fits.open(dud_path) as dud_hdul:\n",
    "            dud_data = next(hdu.data for hdu in dud_hdul if hdu.data is not None)\n",
    "            dud_data = dud_data.astype(np.float32)\n",
    "            dud_data = normalize_dud(dud_data)\n",
    "\n",
    "        # Cargar y normalizar ACS\n",
    "        with fits.open(acs_path) as acs_hdul:\n",
    "            acs_data = next(hdu.data for hdu in acs_hdul if hdu.data is not None)\n",
    "            acs_data = acs_data.astype(np.float32)\n",
    "            acs_data = normalize_acs(acs_data)\n",
    "\n",
    "        # Guardar archivos .npy en sus carpetas respectivas\n",
    "        np.save(os.path.join(dud_output_dir, base_name + \".npy\"), dud_data)\n",
    "        np.save(os.path.join(acs_output_dir, base_name + \".npy\"), acs_data)\n",
    "\n",
    "        paired_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {dud_file}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total de pares procesados y guardados: {paired_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2deee0-d989-4117-996b-aa0f56f619d3",
   "metadata": {},
   "source": [
    "S√≠, otra opci√≥n que a menudo se considera superior para el downsampling (reducci√≥n de tama√±o) en t√©rminos de calidad de imagen es la interpolaci√≥n Lanczos.\n",
    "\n",
    "Lanczos tiende a producir resultados m√°s n√≠tidos y con menos desenfoque que la bic√∫bica, lo que puede ser beneficioso para preservar los detalles finos en las galaxias. Sin embargo, a veces puede introducir un efecto de \"ringing\" (artefactos de anillo) alrededor de bordes muy contrastados, aunque esto rara vez es un problema mayor para la mayor√≠a de las im√°genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e07ebd-a557-4bf0-a444-d189d73d1a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PASO 2: Aplicando Padding/Reescalado Lanczos a datos .npy normalizados ---\n",
      "Directorio de salida '../data/normaliced/dud_processed_lanczos' creado.\n",
      "\n",
      "Procesando im√°genes .npy en '../data/normaliced/dud' a 21x21 (padding/reescalado Lanczos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [00:19<00:00, 842.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de salida '../data/normaliced/acs_processed_lanczos' creado.\n",
      "\n",
      "Procesando im√°genes .npy en '../data/normaliced/acs' a 71x71 (padding/reescalado Lanczos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [00:26<00:00, 614.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ ¬°Procesamiento completo: normalizaci√≥n y reescalado Lanczos aplicados!\n",
      "Im√°genes DUD procesadas guardadas en: ../data/normaliced/dud_processed_lanczos\n",
      "Im√°genes ACS procesadas guardadas en: ../data/normaliced/acs_processed_lanczos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pading y resice de las grandes\n",
    "import os\n",
    "import numpy as np\n",
    "from astropy.io import fits # Todav√≠a se usa si quieres el header, pero no para leer la imagen\n",
    "from scipy.ndimage import zoom\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_normalized_npy(input_npy_dir, output_npy_dir, target_resolution):\n",
    "    \"\"\"\n",
    "    Aplica padding a im√°genes .npy peque√±as y reescalado Lanczos a im√°genes .npy grandes\n",
    "    para alcanzar una resoluci√≥n cuadrada objetivo. Guarda las im√°genes resultantes\n",
    "    como archivos .npy.\n",
    "\n",
    "    Args:\n",
    "        input_npy_dir (str): Directorio de entrada con las im√°genes .npy normalizadas.\n",
    "        output_npy_dir (str): Directorio donde se guardar√°n las im√°genes .npy procesadas.\n",
    "        target_resolution (int): La resoluci√≥n cuadrada deseada (ej. 21 para 21x21).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_npy_dir):\n",
    "        print(f\"Error: El directorio de entrada '{input_npy_dir}' no existe.\")\n",
    "        return\n",
    "    if not os.path.exists(output_npy_dir):\n",
    "        os.makedirs(output_npy_dir)\n",
    "        print(f\"Directorio de salida '{output_npy_dir}' creado.\")\n",
    "\n",
    "    npy_files = [f for f in os.listdir(input_npy_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "    if not npy_files:\n",
    "        print(f\"No se encontraron archivos .npy en '{input_npy_dir}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcesando im√°genes .npy en '{input_npy_dir}' a {target_resolution}x{target_resolution} (padding/reescalado Lanczos)...\")\n",
    "\n",
    "    for filename in tqdm(npy_files):\n",
    "        input_path = os.path.join(input_npy_dir, filename)\n",
    "        output_path = os.path.join(output_npy_dir, filename) # Se mantiene el mismo nombre\n",
    "\n",
    "        try:\n",
    "            original_data = np.load(input_path)\n",
    "            original_height, original_width = original_data.shape\n",
    "            original_dtype = original_data.dtype # Usamos el dtype original, que ser√° float32\n",
    "\n",
    "            processed_data = np.zeros((target_resolution, target_resolution), dtype=original_dtype)\n",
    "\n",
    "            # --- L√≥gica de procesamiento ---\n",
    "            if original_width < target_resolution or original_height < target_resolution:\n",
    "                # Aplicar padding para im√°genes peque√±as\n",
    "                start_y = (target_resolution - original_height) // 2\n",
    "                start_x = (target_resolution - original_width) // 2\n",
    "                \n",
    "                processed_data[start_y : start_y + original_height,\n",
    "                               start_x : start_x + original_width] = original_data\n",
    "            else:\n",
    "                # Reescalar Lanczos para im√°genes grandes o de tama√±o similar\n",
    "                zoom_factor_h = target_resolution / original_height\n",
    "                zoom_factor_w = target_resolution / original_width\n",
    "\n",
    "                # No necesitamos convertir a float32 aqu√≠ porque ya est√°n normalizadas (float32)\n",
    "                zoomed_data = zoom(original_data, (zoom_factor_h, zoom_factor_w), order=5) \n",
    "                \n",
    "                # Aseguramos que la imagen reescalada tenga exactamente la resoluci√≥n objetivo\n",
    "                current_h, current_w = zoomed_data.shape\n",
    "                \n",
    "                if current_h != target_resolution or current_w != target_resolution:\n",
    "                    final_zoomed_data = np.zeros((target_resolution, target_resolution), dtype=original_dtype)\n",
    "                    \n",
    "                    insert_h = min(target_resolution, current_h)\n",
    "                    insert_w = min(target_resolution, current_w)\n",
    "\n",
    "                    start_y_zoom = (target_resolution - insert_h) // 2\n",
    "                    start_x_zoom = (target_resolution - insert_w) // 2\n",
    "\n",
    "                    src_start_y = (current_h - insert_h) // 2\n",
    "                    src_start_x = (current_w - insert_w) // 2\n",
    "\n",
    "                    final_zoomed_data[start_y_zoom : start_y_zoom + insert_h,\n",
    "                                      start_x_zoom : start_x_zoom + insert_w] = \\\n",
    "                                      zoomed_data[src_start_y : src_start_y + insert_h,\n",
    "                                                  src_start_x : src_start_x + insert_w]\n",
    "                    processed_data = final_zoomed_data.astype(original_dtype)\n",
    "                else:\n",
    "                    processed_data = zoomed_data.astype(original_dtype)\n",
    "            \n",
    "            # Guardar el archivo .npy procesado\n",
    "            np.save(output_path, processed_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {filename} (reescalado Lanczos): {e}\")\n",
    "\n",
    "# Directorios de salida para los datos normalizados (.npy)\n",
    "normalized_base_dir = \"../data/normaliced/\"\n",
    "dud_normalized_dir = os.path.join(normalized_base_dir, \"dud\")\n",
    "acs_normalized_dir = os.path.join(normalized_base_dir, \"acs\")\n",
    "\n",
    "# Define los directorios de salida para las im√°genes .npy procesadas\n",
    "# (despu√©s de normalizaci√≥n y reescalado/padding)\n",
    "processed_dud_npy_dir = os.path.join(normalized_base_dir, \"dud_processed_lanczos\")\n",
    "processed_acs_npy_dir = os.path.join(normalized_base_dir, \"acs_processed_lanczos\")\n",
    "\n",
    "# Definir resoluciones objetivo (las mismas que antes)\n",
    "target_dud_resolution = 21\n",
    "target_acs_resolution = 71\n",
    "\n",
    "print(\"\\n--- PASO 2: Aplicando Padding/Reescalado Lanczos a datos .npy normalizados ---\")\n",
    "\n",
    "# Procesar las im√°genes DUD normalizadas\n",
    "process_normalized_npy(dud_normalized_dir, processed_dud_npy_dir, target_dud_resolution)\n",
    "\n",
    "# Procesar las im√°genes ACS normalizadas\n",
    "process_normalized_npy(acs_normalized_dir, processed_acs_npy_dir, target_acs_resolution)\n",
    "\n",
    "print(\"\\nüéâ ¬°Procesamiento completo: normalizaci√≥n y reescalado Lanczos aplicados!\")\n",
    "print(f\"Im√°genes DUD procesadas guardadas en: {processed_dud_npy_dir}\")\n",
    "print(f\"Im√°genes ACS procesadas guardadas en: {processed_acs_npy_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee4b8a80-e6cc-459b-82ce-4c236edc0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PASO 2: Aplicando Padding/Reescalado Lanczos a datos .npy normalizados ---\n",
      "Directorio de salida '../data/normaliced/pading\\dud' creado.\n",
      "\n",
      "Procesando im√°genes .npy en '../data/normaliced/dud' a 152x152 (padding/reescalado Lanczos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [00:16<00:00, 968.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de salida '../data/normaliced/pading\\acs' creado.\n",
      "\n",
      "Procesando im√°genes .npy en '../data/normaliced/acs' a 520x520 (padding/reescalado Lanczos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [00:41<00:00, 390.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ ¬°Procesamiento completo: normalizaci√≥n y reescalado Lanczos aplicados!\n",
      "Im√°genes DUD procesadas guardadas en: ../data/normaliced/pading\\dud\n",
      "Im√°genes ACS procesadas guardadas en: ../data/normaliced/pading\\acs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Solo pading\n",
    "import os\n",
    "import numpy as np\n",
    "from astropy.io import fits # Todav√≠a se usa si quieres el header, pero no para leer la imagen\n",
    "from scipy.ndimage import zoom\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_normalized_npy(input_npy_dir, output_npy_dir, target_resolution):\n",
    "    \"\"\"\n",
    "    Aplica padding a im√°genes .npy peque√±as y reescalado Lanczos a im√°genes .npy grandes\n",
    "    para alcanzar una resoluci√≥n cuadrada objetivo. Guarda las im√°genes resultantes\n",
    "    como archivos .npy.\n",
    "\n",
    "    Args:\n",
    "        input_npy_dir (str): Directorio de entrada con las im√°genes .npy normalizadas.\n",
    "        output_npy_dir (str): Directorio donde se guardar√°n las im√°genes .npy procesadas.\n",
    "        target_resolution (int): La resoluci√≥n cuadrada deseada (ej. 21 para 21x21).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_npy_dir):\n",
    "        print(f\"Error: El directorio de entrada '{input_npy_dir}' no existe.\")\n",
    "        return\n",
    "    if not os.path.exists(output_npy_dir):\n",
    "        os.makedirs(output_npy_dir)\n",
    "        print(f\"Directorio de salida '{output_npy_dir}' creado.\")\n",
    "\n",
    "    npy_files = [f for f in os.listdir(input_npy_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "    if not npy_files:\n",
    "        print(f\"No se encontraron archivos .npy en '{input_npy_dir}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcesando im√°genes .npy en '{input_npy_dir}' a {target_resolution}x{target_resolution} (padding/reescalado Lanczos)...\")\n",
    "\n",
    "    for filename in tqdm(npy_files):\n",
    "        input_path = os.path.join(input_npy_dir, filename)\n",
    "        output_path = os.path.join(output_npy_dir, filename) # Se mantiene el mismo nombre\n",
    "\n",
    "        try:\n",
    "            original_data = np.load(input_path)\n",
    "            original_height, original_width = original_data.shape\n",
    "            original_dtype = original_data.dtype # Usamos el dtype original, que ser√° float32\n",
    "\n",
    "            processed_data = np.zeros((target_resolution, target_resolution), dtype=original_dtype)\n",
    "\n",
    "            # --- L√≥gica de procesamiento ---\n",
    "            if original_width < target_resolution or original_height < target_resolution:\n",
    "                # Aplicar padding para im√°genes peque√±as\n",
    "                start_y = (target_resolution - original_height) // 2\n",
    "                start_x = (target_resolution - original_width) // 2\n",
    "                \n",
    "                processed_data[start_y : start_y + original_height,\n",
    "                               start_x : start_x + original_width] = original_data\n",
    "            else:\n",
    "                # Reescalar Lanczos para im√°genes grandes o de tama√±o similar\n",
    "                zoom_factor_h = target_resolution / original_height\n",
    "                zoom_factor_w = target_resolution / original_width\n",
    "\n",
    "                # No necesitamos convertir a float32 aqu√≠ porque ya est√°n normalizadas (float32)\n",
    "                zoomed_data = zoom(original_data, (zoom_factor_h, zoom_factor_w), order=5) \n",
    "                \n",
    "                # Aseguramos que la imagen reescalada tenga exactamente la resoluci√≥n objetivo\n",
    "                current_h, current_w = zoomed_data.shape\n",
    "                \n",
    "                if current_h != target_resolution or current_w != target_resolution:\n",
    "                    final_zoomed_data = np.zeros((target_resolution, target_resolution), dtype=original_dtype)\n",
    "                    \n",
    "                    insert_h = min(target_resolution, current_h)\n",
    "                    insert_w = min(target_resolution, current_w)\n",
    "\n",
    "                    start_y_zoom = (target_resolution - insert_h) // 2\n",
    "                    start_x_zoom = (target_resolution - insert_w) // 2\n",
    "\n",
    "                    src_start_y = (current_h - insert_h) // 2\n",
    "                    src_start_x = (current_w - insert_w) // 2\n",
    "\n",
    "                    final_zoomed_data[start_y_zoom : start_y_zoom + insert_h,\n",
    "                                      start_x_zoom : start_x_zoom + insert_w] = \\\n",
    "                                      zoomed_data[src_start_y : src_start_y + insert_h,\n",
    "                                                  src_start_x : src_start_x + insert_w]\n",
    "                    processed_data = final_zoomed_data.astype(original_dtype)\n",
    "                else:\n",
    "                    processed_data = zoomed_data.astype(original_dtype)\n",
    "            \n",
    "            # Guardar el archivo .npy procesado\n",
    "            np.save(output_path, processed_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {filename} (reescalado Lanczos): {e}\")\n",
    "\n",
    "# Directorios de salida para los datos normalizados (.npy)\n",
    "normalized_base_dir = \"../data/normaliced/\"\n",
    "dud_normalized_dir = os.path.join(normalized_base_dir, \"dud\")\n",
    "acs_normalized_dir = os.path.join(normalized_base_dir, \"acs\")\n",
    "\n",
    "# Define los directorios de salida para las im√°genes .npy procesadas\n",
    "# (despu√©s de normalizaci√≥n y reescalado/padding)\n",
    "processed_dud_npy_dir = os.path.join(normalized_base_dir,\"pading\", \"dud\")\n",
    "processed_acs_npy_dir = os.path.join(normalized_base_dir,\"pading\", \"acs\")\n",
    "\n",
    "# Definir resoluciones objetivo (las mismas que antes)\n",
    "target_dud_resolution = 152\n",
    "target_acs_resolution = 520\n",
    "\n",
    "print(\"\\n--- PASO 2: Aplicando Padding/Reescalado Lanczos a datos .npy normalizados ---\")\n",
    "\n",
    "# Procesar las im√°genes DUD normalizadas\n",
    "process_normalized_npy(dud_normalized_dir, processed_dud_npy_dir, target_dud_resolution)\n",
    "\n",
    "# Procesar las im√°genes ACS normalizadas\n",
    "process_normalized_npy(acs_normalized_dir, processed_acs_npy_dir, target_acs_resolution)\n",
    "\n",
    "print(\"\\nüéâ ¬°Procesamiento completo: normalizaci√≥n y reescalado Lanczos aplicados!\")\n",
    "print(f\"Im√°genes DUD procesadas guardadas en: {processed_dud_npy_dir}\")\n",
    "print(f\"Im√°genes ACS procesadas guardadas en: {processed_acs_npy_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573f09fb-4214-4695-b0cf-56de4a8df611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aumento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6814225d-c1db-4ede-8860-97ece43d26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rotaci√≥n y flip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8016d4-e2cc-4375-b620-907b9b556199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generando aumentos de datos y copiando originales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16114/16114 [32:29<00:00,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Aumento de datos completado.\n",
      "üîπ Pares originales: 16114\n",
      "üî∏ Datos aumentados: 112798\n",
      "üì¶ Total final de archivos: 128912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directorios\n",
    "input_dud_dir = \"../data/normaliced/pading/dud/\"\n",
    "input_acs_dir = \"../data/normaliced/pading/acs/\"\n",
    "\n",
    "output_dud_dir = \"../data/Augmentation/dud/\"\n",
    "output_acs_dir = \"../data/Augmentation/acs/\"\n",
    "\n",
    "os.makedirs(output_dud_dir, exist_ok=True)\n",
    "os.makedirs(output_acs_dir, exist_ok=True)\n",
    "\n",
    "# Aumentos (funci√≥n, sufijo)\n",
    "augmentations = [\n",
    "    (lambda x: np.rot90(x, 1), \"rot90\"),\n",
    "    (lambda x: np.rot90(x, 2), \"rot180\"),\n",
    "    (lambda x: np.rot90(x, 3), \"rot270\"),\n",
    "    (lambda x: np.flipud(x), \"flipud\"),\n",
    "    (lambda x: np.fliplr(x), \"fliplr\"),\n",
    "    (lambda x: np.flipud(np.rot90(x, 1)), \"flipud_rot90\"),\n",
    "    (lambda x: np.fliplr(np.rot90(x, 1)), \"fliplr_rot90\"),\n",
    "]\n",
    "\n",
    "original_count = 0\n",
    "augmented_count = 0\n",
    "\n",
    "print(\"üîÑ Generando aumentos de datos y copiando originales...\")\n",
    "\n",
    "# Buscar pares v√°lidos\n",
    "dud_files = [f for f in os.listdir(input_dud_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "for dud_file in tqdm(dud_files):\n",
    "    base_name = dud_file.replace(\".npy\", \"\")\n",
    "    acs_file = base_name + \".npy\"\n",
    "\n",
    "    dud_path = os.path.join(input_dud_dir, dud_file)\n",
    "    acs_path = os.path.join(input_acs_dir, acs_file)\n",
    "\n",
    "    if not os.path.exists(acs_path):\n",
    "        print(f\"‚ö†Ô∏è Falta el archivo ACS para {dud_file}\")\n",
    "        continue\n",
    "\n",
    "    # Cargar datos\n",
    "    dud_data = np.load(dud_path)\n",
    "    acs_data = np.load(acs_path)\n",
    "\n",
    "    # Guardar copia original\n",
    "    np.save(os.path.join(output_dud_dir, f\"{base_name}.npy\"), dud_data)\n",
    "    np.save(os.path.join(output_acs_dir, f\"{base_name}.npy\"), acs_data)\n",
    "    original_count += 1\n",
    "\n",
    "    # Aumentos\n",
    "    for i, (aug_fn, suffix) in enumerate(augmentations, start=1):\n",
    "        aug_dud = aug_fn(dud_data)\n",
    "        aug_acs = aug_fn(acs_data)\n",
    "\n",
    "        aug_dud_path = os.path.join(output_dud_dir, f\"{base_name}_aug{i}_{suffix}.npy\")\n",
    "        aug_acs_path = os.path.join(output_acs_dir, f\"{base_name}_aug{i}_{suffix}.npy\")\n",
    "\n",
    "        np.save(aug_dud_path, aug_dud)\n",
    "        np.save(aug_acs_path, aug_acs)\n",
    "        augmented_count += 1\n",
    "\n",
    "# Estad√≠sticas finales\n",
    "total = original_count + augmented_count\n",
    "print(\"\\n‚úÖ Aumento de datos completado.\")\n",
    "print(f\"üîπ Pares originales: {original_count}\")\n",
    "print(f\"üî∏ Datos aumentados: {augmented_count}\")\n",
    "print(f\"üì¶ Total final de archivos: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5293bb-c600-4b7c-b0ea-06a59f1aaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zoom y deslizamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774619d6-bb4e-452d-907a-d3aa5799f534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Generando aumentos por zoom con deslizamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 128912/128912 [4:27:13<00:00,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Zoom con deslizamiento completado.\n",
      "üîπ Originales procesados: 128912\n",
      "üî∏ Zooms generados: 515648\n",
      "üì¶ Total final: 644560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directorios de entrada\n",
    "input_dud_dir = \"../data/Augmentation/dud/\"\n",
    "input_acs_dir = \"../data/Augmentation/acs/\"\n",
    "\n",
    "# Directorios de salida\n",
    "output_dud_dir = \"../data/AugZum/dud/\"\n",
    "output_acs_dir = \"../data/AugZum/acs/\"\n",
    "os.makedirs(output_dud_dir, exist_ok=True)\n",
    "os.makedirs(output_acs_dir, exist_ok=True)\n",
    "\n",
    "# Par√°metros de zoom (2 p√≠xeles de margen)\n",
    "crop_margin = 2\n",
    "original_count = 0\n",
    "augmented_count = 0\n",
    "\n",
    "print(\"üîç Generando aumentos por zoom con deslizamiento...\")\n",
    "\n",
    "dud_files = [f for f in os.listdir(input_dud_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "for file in tqdm(dud_files):\n",
    "    base_name = file.replace(\".npy\", \"\")\n",
    "    dud_path = os.path.join(input_dud_dir, file)\n",
    "    acs_path = os.path.join(input_acs_dir, file)\n",
    "\n",
    "    if not os.path.exists(acs_path):\n",
    "        print(f\"‚ö†Ô∏è Falta el archivo ACS para {file}\")\n",
    "        continue\n",
    "\n",
    "    # Cargar im√°genes\n",
    "    dud = np.load(dud_path)\n",
    "    acs = np.load(acs_path)\n",
    "\n",
    "    h_dud, w_dud = dud.shape\n",
    "    h_acs, w_acs = acs.shape\n",
    "\n",
    "    # Guardar copia original\n",
    "    np.save(os.path.join(output_dud_dir, file), dud)\n",
    "    np.save(os.path.join(output_acs_dir, file), acs)\n",
    "    original_count += 1\n",
    "\n",
    "    # 4 crops con desplazamiento de 2 p√≠xeles\n",
    "    for idx, (dy, dx) in enumerate([(0, 0), (0, crop_margin), (crop_margin, 0), (crop_margin, crop_margin)], start=1):\n",
    "        dud_crop = dud[dy:h_dud - crop_margin + dy, dx:w_dud - crop_margin + dx]\n",
    "        acs_crop = acs[dy:h_acs - crop_margin + dy, dx:w_acs - crop_margin + dx]\n",
    "\n",
    "        dud_out = os.path.join(output_dud_dir, f\"{base_name}_zoom{idx}.npy\")\n",
    "        acs_out = os.path.join(output_acs_dir, f\"{base_name}_zoom{idx}.npy\")\n",
    "\n",
    "        np.save(dud_out, dud_crop)\n",
    "        np.save(acs_out, acs_crop)\n",
    "        augmented_count += 1\n",
    "\n",
    "# Resumen\n",
    "total = original_count + augmented_count\n",
    "print(\"\\n‚úÖ Zoom con deslizamiento completado.\")\n",
    "print(f\"üîπ Originales procesados: {original_count}\")\n",
    "print(f\"üî∏ Zooms generados: {augmented_count}\")\n",
    "print(f\"üì¶ Total final: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd43fbb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and output sizes should be greater than 0, but got input (H: 0, W: 1) output (H: 64, W: 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m acs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(acs_path)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dud\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_shape:\n\u001b[0;32m---> 30\u001b[0m     dud \u001b[38;5;241m=\u001b[39m \u001b[43mresize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdud\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(dud_path, dud)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acs\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_shape:\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mresize_array\u001b[0;34m(array, size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresize_array\u001b[39m(array, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)):\n\u001b[1;32m      7\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(array, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [1, 1, H, W]\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     resized \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resized\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tfm/lib/python3.10/site-packages/torch/nn/functional.py:4580\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4571\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   4572\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_xpu\n\u001b[1;32m   4573\u001b[0m         ):\n\u001b[1;32m   4574\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[1;32m   4575\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4576\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4577\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[1;32m   4578\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4579\u001b[0m             )\u001b[38;5;241m.\u001b[39m_upsample_linear_vec(\u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 4580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_bilinear2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4581\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[1;32m   4582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4584\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and output sizes should be greater than 0, but got input (H: 0, W: 1) output (H: 64, W: 64)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    " \n",
    "def resize_array(array, size=(64, 64)):\n",
    "    tensor = torch.tensor(array, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]\n",
    "    resized = F.interpolate(tensor, size=size, mode='bilinear', align_corners=False)\n",
    "    return resized.squeeze().numpy()\n",
    " \n",
    "# Rutas\n",
    "input_dud_dir = \"../data/AugZum/dud/\"\n",
    "input_acs_dir = \"../data/AugZum/acs/\"\n",
    "expected_shape = (64, 64)\n",
    " \n",
    "fixed_count = 0\n",
    "for filename in sorted(os.listdir(input_dud_dir)):\n",
    "    if filename.endswith(\".npy\"):\n",
    "        dud_path = os.path.join(input_dud_dir, filename)\n",
    "        acs_path = os.path.join(input_acs_dir, filename)\n",
    " \n",
    "        if not os.path.exists(acs_path):\n",
    "            print(f\"Falta el archivo ACS: {filename}\")\n",
    "            continue\n",
    " \n",
    "        dud = np.load(dud_path)\n",
    "        acs = np.load(acs_path)\n",
    " \n",
    "        if dud.shape != expected_shape:\n",
    "            dud = resize_array(dud)\n",
    "            np.save(dud_path, dud)\n",
    " \n",
    "        if acs.shape != expected_shape:\n",
    "            acs = resize_array(acs)\n",
    "            np.save(acs_path, acs)\n",
    " \n",
    "        fixed_count += 1\n",
    " \n",
    "print(f\"\\n Total convertidos a 64x64: {fixed_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c207e1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16): 22784 im√°genes\n",
      "(13, 13): 29624 im√°genes\n",
      "(26, 26): 6344 im√°genes\n",
      "(10, 11): 7076 im√°genes\n",
      "(14, 14): 27472 im√°genes\n",
      "(17, 16): 5376 im√°genes\n",
      "(31, 31): 3768 im√°genes\n",
      "(10, 10): 25392 im√°genes\n",
      "(22, 21): 3544 im√°genes\n",
      "(71, 72): 80 im√°genes\n",
      "(7, 8): 3216 im√°genes\n",
      "(15, 15): 22736 im√°genes\n",
      "(11, 11): 25160 im√°genes\n",
      "(8, 8): 13896 im√°genes\n",
      "(48, 48): 792 im√°genes\n",
      "(14, 13): 7008 im√°genes\n",
      "(30, 30): 4032 im√°genes\n",
      "(9, 9): 23664 im√°genes\n",
      "(11, 12): 6520 im√°genes\n",
      "(6, 7): 1264 im√°genes\n",
      "(18, 18): 14280 im√°genes\n",
      "(12, 13): 5560 im√°genes\n",
      "(31, 32): 820 im√°genes\n",
      "(22, 22): 14496 im√°genes\n",
      "(9, 8): 4672 im√°genes\n",
      "(19, 20): 3828 im√°genes\n",
      "(17, 17): 21712 im√°genes\n",
      "(18, 17): 4120 im√°genes\n",
      "(13, 14): 7008 im√°genes\n",
      "(14, 15): 6328 im√°genes\n",
      "(19, 18): 4580 im√°genes\n",
      "(12, 12): 31968 im√°genes\n",
      "(20, 20): 12968 im√°genes\n",
      "(20, 19): 3828 im√°genes\n",
      "(16, 17): 5376 im√°genes\n",
      "(8, 9): 4672 im√°genes\n",
      "(7, 7): 11304 im√°genes\n",
      "(16, 15): 5292 im√°genes\n",
      "(28, 28): 5168 im√°genes\n",
      "(25, 25): 7336 im√°genes\n",
      "(35, 35): 1968 im√°genes\n",
      "(41, 41): 1728 im√°genes\n",
      "(21, 21): 10576 im√°genes\n",
      "(21, 20): 3836 im√°genes\n",
      "(7, 6): 1264 im√°genes\n",
      "(10, 9): 5584 im√°genes\n",
      "(35, 36): 788 im√°genes\n",
      "(38, 38): 2048 im√°genes\n",
      "(38, 37): 476 im√°genes\n",
      "(37, 37): 2264 im√°genes\n",
      "(19, 19): 18536 im√°genes\n",
      "(24, 25): 1972 im√°genes\n",
      "(32, 32): 4152 im√°genes\n",
      "(149, 150): 16 im√°genes\n",
      "(12, 11): 6520 im√°genes\n",
      "(6, 6): 3680 im√°genes\n",
      "(15, 14): 6328 im√°genes\n",
      "(97, 97): 40 im√°genes\n",
      "(41, 42): 280 im√°genes\n",
      "(64, 64): 312 im√°genes\n",
      "(5, 5): 968 im√°genes\n",
      "(8, 7): 3216 im√°genes\n",
      "(44, 45): 284 im√°genes\n",
      "(18, 19): 4580 im√°genes\n",
      "(76, 76): 312 im√°genes\n",
      "(15, 16): 5292 im√°genes\n",
      "(29, 29): 5912 im√°genes\n",
      "(25, 26): 1976 im√°genes\n",
      "(20, 21): 3836 im√°genes\n",
      "(23, 24): 2392 im√°genes\n",
      "(44, 44): 1016 im√°genes\n",
      "(43, 44): 280 im√°genes\n",
      "(36, 36): 2520 im√°genes\n",
      "(39, 39): 1624 im√°genes\n",
      "(82, 81): 72 im√°genes\n",
      "(25, 24): 1972 im√°genes\n",
      "(54, 54): 672 im√°genes\n",
      "(40, 40): 2120 im√°genes\n",
      "(27, 28): 1060 im√°genes\n",
      "(9, 10): 5584 im√°genes\n",
      "(36, 37): 400 im√°genes\n",
      "(33, 32): 808 im√°genes\n",
      "(44, 43): 280 im√°genes\n",
      "(11, 10): 7076 im√°genes\n",
      "(24, 24): 9144 im√°genes\n",
      "(70, 70): 304 im√°genes\n",
      "(43, 42): 256 im√°genes\n",
      "(41, 40): 392 im√°genes\n",
      "(28, 29): 1168 im√°genes\n",
      "(46, 46): 1032 im√°genes\n",
      "(103, 103): 16 im√°genes\n",
      "(32, 33): 808 im√°genes\n",
      "(13, 12): 5560 im√°genes\n",
      "(5, 6): 856 im√°genes\n",
      "(24, 23): 2392 im√°genes\n",
      "(31, 30): 1024 im√°genes\n",
      "(47, 48): 280 im√°genes\n",
      "(23, 23): 8064 im√°genes\n",
      "(33, 33): 2512 im√°genes\n",
      "(60, 59): 60 im√°genes\n",
      "(30, 31): 1024 im√°genes\n",
      "(81, 81): 280 im√°genes\n",
      "(40, 39): 304 im√°genes\n",
      "(17, 18): 4120 im√°genes\n",
      "(74, 73): 128 im√°genes\n",
      "(33, 34): 816 im√°genes\n",
      "(51, 52): 88 im√°genes\n",
      "(49, 48): 212 im√°genes\n",
      "(34, 34): 3248 im√°genes\n",
      "(42, 43): 256 im√°genes\n",
      "(49, 50): 220 im√°genes\n",
      "(21, 22): 3544 im√°genes\n",
      "(72, 73): 56 im√°genes\n",
      "(93, 93): 72 im√°genes\n",
      "(34, 35): 792 im√°genes\n",
      "(27, 26): 1488 im√°genes\n",
      "(27, 27): 6784 im√°genes\n",
      "(32, 31): 820 im√°genes\n",
      "(55, 56): 148 im√°genes\n",
      "(43, 43): 1096 im√°genes\n",
      "(26, 25): 1976 im√°genes\n",
      "(22, 23): 2224 im√°genes\n",
      "(34, 33): 816 im√°genes\n",
      "(62, 62): 456 im√°genes\n",
      "(42, 42): 1808 im√°genes\n",
      "(45, 45): 1432 im√°genes\n",
      "(101, 101): 80 im√°genes\n",
      "(60, 60): 472 im√°genes\n",
      "(4, 4): 264 im√°genes\n",
      "(51, 51): 600 im√°genes\n",
      "(48, 47): 280 im√°genes\n",
      "(40, 41): 392 im√°genes\n",
      "(120, 121): 16 im√°genes\n",
      "(72, 71): 80 im√°genes\n",
      "(69, 69): 448 im√°genes\n",
      "(30, 29): 888 im√°genes\n",
      "(47, 47): 984 im√°genes\n",
      "(74, 74): 264 im√°genes\n",
      "(78, 77): 36 im√°genes\n",
      "(50, 49): 220 im√°genes\n",
      "(3, 3): 168 im√°genes\n",
      "(23, 22): 2224 im√°genes\n",
      "(28, 27): 1060 im√°genes\n",
      "(59, 60): 60 im√°genes\n",
      "(47, 46): 268 im√°genes\n",
      "(69, 70): 80 im√°genes\n",
      "(84, 84): 48 im√°genes\n",
      "(67, 67): 352 im√°genes\n",
      "(53, 53): 336 im√°genes\n",
      "(59, 59): 408 im√°genes\n",
      "(36, 35): 788 im√°genes\n",
      "(52, 51): 88 im√°genes\n",
      "(29, 28): 1168 im√°genes\n",
      "(71, 70): 44 im√°genes\n",
      "(58, 57): 140 im√°genes\n",
      "(26, 27): 1488 im√°genes\n",
      "(78, 78): 160 im√°genes\n",
      "(6, 5): 856 im√°genes\n",
      "(35, 34): 792 im√°genes\n",
      "(39, 38): 348 im√°genes\n",
      "(50, 50): 720 im√°genes\n",
      "(110, 109): 16 im√°genes\n",
      "(117, 117): 48 im√°genes\n",
      "(53, 52): 184 im√°genes\n",
      "(46, 47): 268 im√°genes\n",
      "(65, 64): 72 im√°genes\n",
      "(65, 65): 416 im√°genes\n",
      "(58, 58): 288 im√°genes\n",
      "(39, 40): 304 im√°genes\n",
      "(73, 74): 128 im√°genes\n",
      "(79, 80): 40 im√°genes\n",
      "(75, 75): 328 im√°genes\n",
      "(79, 79): 304 im√°genes\n",
      "(45, 44): 284 im√°genes\n",
      "(123, 122): 20 im√°genes\n",
      "(52, 52): 784 im√°genes\n",
      "(4, 3): 96 im√°genes\n",
      "(71, 71): 256 im√°genes\n",
      "(112, 111): 4 im√°genes\n",
      "(77, 77): 400 im√°genes\n",
      "(37, 38): 476 im√°genes\n",
      "(49, 49): 776 im√°genes\n",
      "(37, 36): 400 im√°genes\n",
      "(81, 82): 72 im√°genes\n",
      "(3, 2): 68 im√°genes\n",
      "(51, 50): 136 im√°genes\n",
      "(45, 46): 216 im√°genes\n",
      "(55, 54): 136 im√°genes\n",
      "(94, 94): 88 im√°genes\n",
      "(74, 75): 44 im√°genes\n",
      "(38, 39): 348 im√°genes\n",
      "(42, 41): 280 im√°genes\n",
      "(72, 72): 224 im√°genes\n",
      "(67, 66): 60 im√°genes\n",
      "(85, 84): 52 im√°genes\n",
      "(80, 80): 152 im√°genes\n",
      "(60, 61): 100 im√°genes\n",
      "(67, 68): 72 im√°genes\n",
      "(83, 83): 120 im√°genes\n",
      "(68, 69): 60 im√°genes\n",
      "(89, 89): 96 im√°genes\n",
      "(107, 107): 40 im√°genes\n",
      "(73, 73): 72 im√°genes\n",
      "(55, 55): 344 im√°genes\n",
      "(57, 57): 424 im√°genes\n",
      "(119, 120): 4 im√°genes\n",
      "(76, 75): 44 im√°genes\n",
      "(82, 82): 96 im√°genes\n",
      "(54, 53): 156 im√°genes\n",
      "(29, 30): 888 im√°genes\n",
      "(68, 68): 248 im√°genes\n",
      "(96, 96): 112 im√°genes\n",
      "(73, 72): 56 im√°genes\n",
      "(66, 66): 280 im√°genes\n",
      "(120, 119): 4 im√°genes\n",
      "(98, 98): 88 im√°genes\n",
      "(4, 5): 208 im√°genes\n",
      "(95, 94): 36 im√°genes\n",
      "(56, 56): 384 im√°genes\n",
      "(77, 78): 36 im√°genes\n",
      "(92, 92): 96 im√°genes\n",
      "(84, 83): 32 im√°genes\n",
      "(50, 51): 136 im√°genes\n",
      "(61, 62): 72 im√°genes\n",
      "(2, 3): 68 im√°genes\n",
      "(63, 62): 112 im√°genes\n",
      "(103, 104): 32 im√°genes\n",
      "(87, 87): 136 im√°genes\n",
      "(87, 88): 20 im√°genes\n",
      "(61, 60): 100 im√°genes\n",
      "(3, 4): 96 im√°genes\n",
      "(69, 68): 60 im√°genes\n",
      "(122, 123): 20 im√°genes\n",
      "(5, 4): 208 im√°genes\n",
      "(63, 63): 176 im√°genes\n",
      "(83, 84): 32 im√°genes\n",
      "(48, 49): 212 im√°genes\n",
      "(86, 85): 20 im√°genes\n",
      "(58, 59): 172 im√°genes\n",
      "(46, 45): 216 im√°genes\n",
      "(56, 57): 136 im√°genes\n",
      "(94, 95): 36 im√°genes\n",
      "(66, 67): 60 im√°genes\n",
      "(114, 114): 40 im√°genes\n",
      "(52, 53): 184 im√°genes\n",
      "(1, 1): 32 im√°genes\n",
      "(53, 54): 156 im√°genes\n",
      "(64, 65): 72 im√°genes\n",
      "(91, 91): 48 im√°genes\n",
      "(104, 104): 40 im√°genes\n",
      "(68, 67): 72 im√°genes\n",
      "(61, 61): 272 im√°genes\n",
      "(115, 115): 64 im√°genes\n",
      "(112, 112): 40 im√°genes\n",
      "(127, 126): 4 im√°genes\n",
      "(57, 58): 140 im√°genes\n",
      "(70, 71): 44 im√°genes\n",
      "(62, 63): 112 im√°genes\n",
      "(89, 90): 20 im√°genes\n",
      "(95, 95): 48 im√°genes\n",
      "(138, 138): 40 im√°genes\n",
      "(2, 2): 32 im√°genes\n",
      "(110, 110): 48 im√°genes\n",
      "(56, 55): 148 im√°genes\n",
      "(76, 77): 8 im√°genes\n",
      "(54, 55): 136 im√°genes\n",
      "(59, 58): 172 im√°genes\n",
      "(116, 115): 16 im√°genes\n",
      "(136, 136): 32 im√°genes\n",
      "(129, 129): 32 im√°genes\n",
      "(88, 88): 48 im√°genes\n",
      "(95, 96): 16 im√°genes\n",
      "(91, 92): 36 im√°genes\n",
      "(92, 91): 36 im√°genes\n",
      "(93, 92): 20 im√°genes\n",
      "(132, 132): 32 im√°genes\n",
      "(150, 150): 32 im√°genes\n",
      "(57, 56): 136 im√°genes\n",
      "(97, 96): 8 im√°genes\n",
      "(64, 63): 64 im√°genes\n",
      "(125, 126): 16 im√°genes\n",
      "(66, 65): 44 im√°genes\n",
      "(105, 105): 32 im√°genes\n",
      "(99, 99): 72 im√°genes\n",
      "(102, 102): 40 im√°genes\n",
      "(70, 69): 80 im√°genes\n",
      "(108, 108): 64 im√°genes\n",
      "(90, 91): 20 im√°genes\n",
      "(107, 106): 16 im√°genes\n",
      "(86, 87): 28 im√°genes\n",
      "(62, 61): 72 im√°genes\n",
      "(105, 106): 8 im√°genes\n",
      "(133, 133): 16 im√°genes\n",
      "(86, 86): 72 im√°genes\n",
      "(65, 66): 44 im√°genes\n",
      "(85, 85): 48 im√°genes\n",
      "(89, 88): 20 im√°genes\n",
      "(80, 79): 40 im√°genes\n",
      "(87, 86): 28 im√°genes\n",
      "(104, 103): 32 im√°genes\n",
      "(90, 89): 20 im√°genes\n",
      "(1, 0): 16 im√°genes\n",
      "(88, 89): 20 im√°genes\n",
      "(152, 151): 4 im√°genes\n",
      "(125, 124): 20 im√°genes\n",
      "(115, 116): 16 im√°genes\n",
      "(85, 86): 20 im√°genes\n",
      "(99, 98): 16 im√°genes\n",
      "(100, 100): 48 im√°genes\n",
      "(118, 117): 20 im√°genes\n",
      "(75, 74): 44 im√°genes\n",
      "(63, 64): 64 im√°genes\n",
      "(101, 100): 4 im√°genes\n",
      "(91, 90): 20 im√°genes\n",
      "(106, 107): 16 im√°genes\n",
      "(131, 131): 72 im√°genes\n",
      "(109, 109): 8 im√°genes\n",
      "(84, 85): 52 im√°genes\n",
      "(90, 90): 8 im√°genes\n",
      "(79, 78): 16 im√°genes\n",
      "(152, 152): 8 im√°genes\n",
      "(96, 97): 8 im√°genes\n",
      "(116, 116): 8 im√°genes\n",
      "(134, 134): 8 im√°genes\n",
      "(151, 152): 4 im√°genes\n",
      "(94, 93): 8 im√°genes\n",
      "(77, 76): 8 im√°genes\n",
      "(93, 94): 8 im√°genes\n",
      "(81, 80): 4 im√°genes\n",
      "(75, 76): 44 im√°genes\n",
      "(126, 127): 4 im√°genes\n",
      "(92, 93): 20 im√°genes\n",
      "(106, 106): 8 im√°genes\n",
      "(98, 97): 4 im√°genes\n",
      "(140, 140): 8 im√°genes\n",
      "(80, 81): 4 im√°genes\n",
      "(127, 128): 4 im√°genes\n",
      "(124, 125): 20 im√°genes\n",
      "(106, 105): 8 im√°genes\n",
      "(97, 98): 4 im√°genes\n",
      "(128, 127): 4 im√°genes\n",
      "(119, 119): 8 im√°genes\n",
      "(96, 95): 16 im√°genes\n",
      "(88, 87): 20 im√°genes\n",
      "(121, 120): 16 im√°genes\n",
      "(78, 79): 16 im√°genes\n",
      "(150, 149): 16 im√°genes\n",
      "(117, 118): 20 im√°genes\n",
      "(0, 1): 16 im√°genes\n",
      "(109, 108): 4 im√°genes\n",
      "(109, 110): 16 im√°genes\n",
      "(82, 83): 16 im√°genes\n",
      "(108, 109): 4 im√°genes\n",
      "(126, 125): 16 im√°genes\n",
      "(100, 101): 4 im√°genes\n",
      "(111, 112): 4 im√°genes\n",
      "(98, 99): 16 im√°genes\n",
      "(83, 82): 16 im√°genes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    " \n",
    "folder_path = \"../data/AugZum/dud\"  \n",
    " \n",
    "shapes = []\n",
    " \n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".npy\"):\n",
    "        path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            arr = np.load(path)\n",
    "            shapes.append(arr.shape)\n",
    "        except Exception as e:\n",
    "            print(f\"Error leyendo {file}: {e}\")\n",
    " \n",
    "# Mostrar cu√°ntas im√°genes hay de cada tama√±o\n",
    "conteo = Counter(shapes)\n",
    "for shape, count in conteo.items():\n",
    "    print(f\"{shape}: {count} im√°genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76942b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
